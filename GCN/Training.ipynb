{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[CXR]_5_Training.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"992fd95bc77440b7954fa0e430dd277b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f9d09e1e6fea4fce926e740bbaf6f160","IPY_MODEL_691b628693894ca9acd1582dff7615bf","IPY_MODEL_d759a38b1c834feea1280598625cea2f"],"layout":"IPY_MODEL_c5a48bd84add4d3c875b7739e5c9f312"}},"f9d09e1e6fea4fce926e740bbaf6f160":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_982c7d6eeeba411ca0c2e180fbb4407e","placeholder":"​","style":"IPY_MODEL_8ee99c6113b54b76942b077e7ef49664","value":"100%"}},"691b628693894ca9acd1582dff7615bf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc9f492d221a40cf829d8165661d4f85","max":500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d8b43f832fd542739b84131c4e1954c2","value":500}},"d759a38b1c834feea1280598625cea2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e141c14a42c4ea8a24196226b137452","placeholder":"​","style":"IPY_MODEL_68bdcca9b1bd4477a70fd32b4c929838","value":" 500/500 [23:26&lt;00:00,  2.79s/it]"}},"c5a48bd84add4d3c875b7739e5c9f312":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"982c7d6eeeba411ca0c2e180fbb4407e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ee99c6113b54b76942b077e7ef49664":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc9f492d221a40cf829d8165661d4f85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8b43f832fd542739b84131c4e1954c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e141c14a42c4ea8a24196226b137452":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68bdcca9b1bd4477a70fd32b4c929838":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"49879c885f814fe4bd212dcb397dc1b2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a1a19950aef44ddba37ec534b4059695","IPY_MODEL_556544f37a0343199278477b814f2b75","IPY_MODEL_d2094f7042e24334bf6f403ed1404423"],"layout":"IPY_MODEL_abc8edb9de964a7c8fa6f949fdad48a5"}},"a1a19950aef44ddba37ec534b4059695":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6325b36008a941f2bba05363629aa142","placeholder":"​","style":"IPY_MODEL_10007e275b1847049902a1bab93378c5","value":"  0%"}},"556544f37a0343199278477b814f2b75":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_63bd031212534fd98c3a0ed859e4c724","max":16,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d04f2a32deb84eef8b0104fcbdb6b3ed","value":0}},"d2094f7042e24334bf6f403ed1404423":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a09ed2baeff4104883bfa30296c63dc","placeholder":"​","style":"IPY_MODEL_5ad48f9ecf1e4913a42d9f99c9c4d08d","value":" 0/16 [00:00&lt;?, ?it/s]"}},"abc8edb9de964a7c8fa6f949fdad48a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6325b36008a941f2bba05363629aa142":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10007e275b1847049902a1bab93378c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"63bd031212534fd98c3a0ed859e4c724":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d04f2a32deb84eef8b0104fcbdb6b3ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9a09ed2baeff4104883bfa30296c63dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ad48f9ecf1e4913a42d9f99c9c4d08d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"6oOOzBSNgnOU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652890985216,"user_tz":-420,"elapsed":3580,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}},"outputId":"57deb883-0725-4cb9-b681-c0620e8b2225"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install torch_geometric torch_sparse torch_scatter"],"metadata":{"id":"pyDVj909jqCy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652890988289,"user_tz":-420,"elapsed":3089,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}},"outputId":"55b68ff4-d3cc-4dd0-b591-78fe4dcac784"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch_geometric in /usr/local/lib/python3.7/dist-packages (2.0.4)\n","Requirement already satisfied: torch_sparse in /usr/local/lib/python3.7/dist-packages (0.6.13)\n","Requirement already satisfied: torch_scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.23.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.0.2)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (3.0.9)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (4.64.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.3.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.4.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.11.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch_geometric) (2.0.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch_geometric) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch_geometric) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch_geometric) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (1.24.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n"]}]},{"cell_type":"markdown","source":["### Import libraries"],"metadata":{"id":"IVwBGT8q9UIW"}},{"cell_type":"code","source":["import os\n","import cv2\n","import glob\n","import numpy as np\n","import pandas as pd\n","import os.path as osp \n","import matplotlib\n","import matplotlib.pyplot as plt\n"," \n","import tqdm.notebook as tqdm\n","from multiprocessing.pool import Pool\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"],"metadata":{"id":"wpzyWYLXiwYB","executionInfo":{"status":"ok","timestamp":1652890990088,"user_tz":-420,"elapsed":1806,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import time\n","import torch\n","import torch.nn.functional as F\n","from torch.nn import Linear, LogSoftmax \n","\n","from torch_geometric.loader import DataLoader\n","from torch_geometric.profile import get_model_size, get_data_size, count_parameters, profileit, timeit\n","from torch_geometric.nn import GATConv, GCNConv, GINConv, GraphConv, global_add_pool, global_mean_pool\n","\n","\n","from sklearn.metrics import classification_report\n","from torch_geometric.data import InMemoryDataset\n","from torch_geometric.io import read_tu_data"],"metadata":{"id":"K4S6b7qRjDw_","executionInfo":{"status":"ok","timestamp":1652890991590,"user_tz":-420,"elapsed":824,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Create Graphs Dataset"],"metadata":{"id":"jQQ9teaE9WMU"}},{"cell_type":"code","source":["class GraphDataset(InMemoryDataset):\n","    def __init__(self, root, name, transform=None, pre_transform=None,\n","                 pre_filter=None, use_node_attr=False, use_edge_attr=False):\n","        self.name = name\n","        super(GraphDataset, self).__init__(root, transform, pre_transform, pre_filter)\n","        print('=======================================================')\n","        print(self.processed_paths[0])\n","        print('=======================================================')\n","        self.data, self.slices = torch.load(self.processed_paths[0]) \n","        \n","    @property\n","    def raw_dir(self):\n","        name = 'raw{}'.format('')\n","        return osp.join(self.root, self.name, name)\n","\n","    @property\n","    def processed_dir(self):\n","        name = 'processed{}'.format('')\n","        return osp.join(self.root, self.name, name)\n","\n","    @property\n","    def num_node_labels(self):\n","        if self.data.x is None:\n","            return 0\n","        for i in range(self.data.x.size(1)):\n","            x = self.data.x[:, i:]\n","            if ((x == 0) | (x == 1)).all() and (x.sum(dim=1) == 1).all():\n","                return self.data.x.size(1) - i\n","        return 0\n","\n","    @property\n","    def num_node_attributes(self):\n","        if self.data.x is None:\n","            return 0\n","        return self.data.x.size(1) - self.num_node_labels\n","\n","    @property\n","    def num_edge_labels(self):\n","        if self.data.edge_attr is None:\n","            return 0\n","        for i in range(self.data.edge_attr.size(1)):\n","            if self.data.edge_attr[:, i:].sum() == self.data.edge_attr.size(0):\n","                return self.data.edge_attr.size(1) - i\n","        return 0\n","\n","    @property\n","    def num_edge_attributes(self):\n","        if self.data.edge_attr is None:\n","            return 0\n","        return self.data.edge_attr.size(1) - self.num_edge_labels\n","\n","    @property\n","    def raw_file_names(self):\n","        names = ['A', 'graph_indicator','graph_labels','node_attributes','node_labels']\n","        return ['{}_{}.txt'.format(self.name, name) for name in names]\n","\n","    @property\n","    def processed_file_names(self):\n","        return 'data.pt'\n","\n","    def process(self):\n","        print('Self - raw_dir: ', self.raw_dir) \n","        print('Self - name: ', self.name)\n","        self.data, self.slices = read_tu_data(self.raw_dir, self.name)\n","        if self.pre_filter is not None:\n","            data_list = [self.get(idx) for idx in range(len(self))]\n","            data_list = [data for data in data_list if self.pre_filter(data)]\n","            self.data, self.slices = self.collate(data_list)\n","\n","        if self.pre_transform is not None:\n","            data_list = [self.get(idx) for idx in range(len(self))]\n","            data_list = [self.pre_transform(data) for data in data_list]\n","            self.data, self.slices = self.collate(data_list)\n","\n","        torch.save((self.data, self.slices), self.processed_paths[0])\n","\n","    def __repr__(self):\n","        return '{}({})'.format(self.name, len(self))"],"metadata":{"id":"5F99yRZnjlxb","executionInfo":{"status":"ok","timestamp":1652890991591,"user_tz":-420,"elapsed":6,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","def parse_txt_array(src, sep=None, start=0, end=None, dtype=None, device=None):\n","    to_number = int\n","    if torch.is_floating_point(torch.empty(0, dtype=dtype)):\n","        to_number = float\n","\n","    __src__ = []   \n","    for line in src: \n","        __arr__ = []  \n","        for x in line.split(sep)[start:end]:\n","            try: \n","                __arr__.append(to_number(x))\n","            except: \n","                __arr__.append(0)\n","        __src__.append(__arr__)  \n","    src = torch.tensor(__src__).to(dtype).squeeze()\n","    return src\n","```\n","\n"],"metadata":{"id":"1C5_tPNYd5pE"}},{"cell_type":"code","source":["root = '/content/drive/MyDrive/COURSES/CS331/FinalProject/dataset/GraphCXR/CXR_Reformat_128_CXR_Prewitt/'\n","name_dataset = 'CXR_Reformat_128_CXR_Prewitt_Trainv2'\n","\n","dataset_train = GraphDataset(root=root, name=name_dataset, use_node_attr=True) \n"," \n","print(f'Dataset name: {dataset_train}:')\n","print('==================')\n","print(f'Number of graphs: {len(dataset_train)}')\n","print(f'Number of features: {dataset_train.num_features}')\n","print(f'Number of classes: {dataset_train.num_classes}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nfM9OrulnuuB","outputId":"1f7b252f-dfb9-4a8d-8336-a8773a91d035","executionInfo":{"status":"ok","timestamp":1652891000879,"user_tz":-420,"elapsed":9293,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["=======================================================\n","/content/drive/MyDrive/COURSES/CS331/FinalProject/dataset/GraphCXR/CXR_Reformat_128_CXR_Prewitt/CXR_Reformat_128_CXR_Prewitt_Trainv2/processed/data.pt\n","=======================================================\n","Dataset name: CXR_Reformat_128_CXR_Prewitt_Trainv2(4022):\n","==================\n","Number of graphs: 4022\n","Number of features: 4\n","Number of classes: 3\n"]}]},{"cell_type":"code","source":["root = '/content/drive/MyDrive/COURSES/CS331/FinalProject/dataset/GraphCXR/CXR_Reformat_128_CXR_Prewitt/'\n","name_dataset = 'CXR_Reformat_128_CXR_Prewitt_Valv2'\n","\n","dataset_test = GraphDataset(root=root, name=name_dataset, use_node_attr=True) \n"," \n","print(f'Dataset name: {dataset_test}:')\n","print('==================')\n","print(f'Number of graphs: {len(dataset_test)}')\n","print(f'Number of features: {dataset_test.num_features}')\n","print(f'Number of classes: {dataset_test.num_classes}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vVrLpsmz26Ig","executionInfo":{"status":"ok","timestamp":1652891003676,"user_tz":-420,"elapsed":2804,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}},"outputId":"916f2fcf-1447-40ef-e2e5-a96176a46775"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["=======================================================\n","/content/drive/MyDrive/COURSES/CS331/FinalProject/dataset/GraphCXR/CXR_Reformat_128_CXR_Prewitt/CXR_Reformat_128_CXR_Prewitt_Valv2/processed/data.pt\n","=======================================================\n","Dataset name: CXR_Reformat_128_CXR_Prewitt_Valv2(994):\n","==================\n","Number of graphs: 994\n","Number of features: 4\n","Number of classes: 3\n"]}]},{"cell_type":"code","source":["root = '/content/drive/MyDrive/COURSES/CS331/FinalProject/dataset/GraphCXR/CXR_Reformat_128_CXR_Prewitt/'\n","name_dataset = 'CXR_Reformat_128_CXR_Prewitt_Testv2'\n","\n","dataset_val = GraphDataset(root=root, name=name_dataset, use_node_attr=True) \n"," \n","print(f'Dataset name: {dataset_val}:')\n","print('==================')\n","print(f'Number of graphs: {len(dataset_val)}')\n","print(f'Number of features: {dataset_val.num_features}')\n","print(f'Number of classes: {dataset_val.num_classes}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"avxaJ-LP3E1P","executionInfo":{"status":"ok","timestamp":1652891006562,"user_tz":-420,"elapsed":2911,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}},"outputId":"199c9d96-5baf-4196-ce8a-0e59d5b699b1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["=======================================================\n","/content/drive/MyDrive/COURSES/CS331/FinalProject/dataset/GraphCXR/CXR_Reformat_128_CXR_Prewitt/CXR_Reformat_128_CXR_Prewitt_Testv2/processed/data.pt\n","=======================================================\n","Dataset name: CXR_Reformat_128_CXR_Prewitt_Testv2(614):\n","==================\n","Number of graphs: 614\n","Number of features: 4\n","Number of classes: 3\n"]}]},{"cell_type":"code","source":["torch.manual_seed(12345)\n","dataset_train = dataset_train.shuffle() \n","dataset_val = dataset_val.shuffle() \n","\n","train_dataset = dataset_train[:]\n","val_dataset = dataset_val[:]\n","test_dataset = dataset_test[:]\n","\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","for step, data in enumerate(train_loader):\n","    print(f'Step {step + 1}:')\n","    print('=======')\n","    print(f'Number of graphs in the current batch: {data.num_graphs}')\n","    print(data)\n","    print()"],"metadata":{"id":"iEZEw8Rgd9jF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652891008903,"user_tz":-420,"elapsed":2352,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}},"outputId":"ac9e910f-c427-4162-9adc-4100fb0ed8f4"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Step 1:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 115414], x=[25148, 4], y=[64], batch=[25148], ptr=[65])\n","\n","Step 2:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 113054], x=[24421, 4], y=[64], batch=[24421], ptr=[65])\n","\n","Step 3:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 121800], x=[27697, 4], y=[64], batch=[27697], ptr=[65])\n","\n","Step 4:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 124612], x=[27255, 4], y=[64], batch=[27255], ptr=[65])\n","\n","Step 5:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 109608], x=[23622, 4], y=[64], batch=[23622], ptr=[65])\n","\n","Step 6:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 101950], x=[22579, 4], y=[64], batch=[22579], ptr=[65])\n","\n","Step 7:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 105660], x=[24244, 4], y=[64], batch=[24244], ptr=[65])\n","\n","Step 8:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 104960], x=[23091, 4], y=[64], batch=[23091], ptr=[65])\n","\n","Step 9:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 108682], x=[24561, 4], y=[64], batch=[24561], ptr=[65])\n","\n","Step 10:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 107734], x=[24024, 4], y=[64], batch=[24024], ptr=[65])\n","\n","Step 11:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 100030], x=[22604, 4], y=[64], batch=[22604], ptr=[65])\n","\n","Step 12:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 96548], x=[21101, 4], y=[64], batch=[21101], ptr=[65])\n","\n","Step 13:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 93926], x=[21203, 4], y=[64], batch=[21203], ptr=[65])\n","\n","Step 14:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 94074], x=[21330, 4], y=[64], batch=[21330], ptr=[65])\n","\n","Step 15:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 104102], x=[22669, 4], y=[64], batch=[22669], ptr=[65])\n","\n","Step 16:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 105442], x=[23311, 4], y=[64], batch=[23311], ptr=[65])\n","\n","Step 17:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 118096], x=[25830, 4], y=[64], batch=[25830], ptr=[65])\n","\n","Step 18:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 103946], x=[22877, 4], y=[64], batch=[22877], ptr=[65])\n","\n","Step 19:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 124172], x=[27160, 4], y=[64], batch=[27160], ptr=[65])\n","\n","Step 20:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 113968], x=[24203, 4], y=[64], batch=[24203], ptr=[65])\n","\n","Step 21:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 136342], x=[29370, 4], y=[64], batch=[29370], ptr=[65])\n","\n","Step 22:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 128394], x=[27823, 4], y=[64], batch=[27823], ptr=[65])\n","\n","Step 23:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 95930], x=[21308, 4], y=[64], batch=[21308], ptr=[65])\n","\n","Step 24:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 112938], x=[24359, 4], y=[64], batch=[24359], ptr=[65])\n","\n","Step 25:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 106960], x=[23154, 4], y=[64], batch=[23154], ptr=[65])\n","\n","Step 26:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 108596], x=[24793, 4], y=[64], batch=[24793], ptr=[65])\n","\n","Step 27:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 131770], x=[28329, 4], y=[64], batch=[28329], ptr=[65])\n","\n","Step 28:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 116328], x=[24639, 4], y=[64], batch=[24639], ptr=[65])\n","\n","Step 29:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 100296], x=[22281, 4], y=[64], batch=[22281], ptr=[65])\n","\n","Step 30:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 123828], x=[26712, 4], y=[64], batch=[26712], ptr=[65])\n","\n","Step 31:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 95100], x=[21098, 4], y=[64], batch=[21098], ptr=[65])\n","\n","Step 32:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 107586], x=[24116, 4], y=[64], batch=[24116], ptr=[65])\n","\n","Step 33:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 129984], x=[28184, 4], y=[64], batch=[28184], ptr=[65])\n","\n","Step 34:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 94560], x=[20681, 4], y=[64], batch=[20681], ptr=[65])\n","\n","Step 35:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 103962], x=[23441, 4], y=[64], batch=[23441], ptr=[65])\n","\n","Step 36:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 148134], x=[30641, 4], y=[64], batch=[30641], ptr=[65])\n","\n","Step 37:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 95296], x=[20714, 4], y=[64], batch=[20714], ptr=[65])\n","\n","Step 38:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 102816], x=[22912, 4], y=[64], batch=[22912], ptr=[65])\n","\n","Step 39:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 119724], x=[26764, 4], y=[64], batch=[26764], ptr=[65])\n","\n","Step 40:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 113232], x=[24982, 4], y=[64], batch=[24982], ptr=[65])\n","\n","Step 41:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 120562], x=[25766, 4], y=[64], batch=[25766], ptr=[65])\n","\n","Step 42:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 120832], x=[26428, 4], y=[64], batch=[26428], ptr=[65])\n","\n","Step 43:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 116048], x=[25034, 4], y=[64], batch=[25034], ptr=[65])\n","\n","Step 44:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 117158], x=[25729, 4], y=[64], batch=[25729], ptr=[65])\n","\n","Step 45:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 120152], x=[26523, 4], y=[64], batch=[26523], ptr=[65])\n","\n","Step 46:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 117338], x=[25314, 4], y=[64], batch=[25314], ptr=[65])\n","\n","Step 47:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 97144], x=[21400, 4], y=[64], batch=[21400], ptr=[65])\n","\n","Step 48:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 100740], x=[22824, 4], y=[64], batch=[22824], ptr=[65])\n","\n","Step 49:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 103554], x=[22481, 4], y=[64], batch=[22481], ptr=[65])\n","\n","Step 50:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 106748], x=[24602, 4], y=[64], batch=[24602], ptr=[65])\n","\n","Step 51:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 111028], x=[24664, 4], y=[64], batch=[24664], ptr=[65])\n","\n","Step 52:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 126508], x=[27402, 4], y=[64], batch=[27402], ptr=[65])\n","\n","Step 53:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 115872], x=[25152, 4], y=[64], batch=[25152], ptr=[65])\n","\n","Step 54:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 124926], x=[27498, 4], y=[64], batch=[27498], ptr=[65])\n","\n","Step 55:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 127912], x=[26915, 4], y=[64], batch=[26915], ptr=[65])\n","\n","Step 56:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 105376], x=[23346, 4], y=[64], batch=[23346], ptr=[65])\n","\n","Step 57:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 92502], x=[20801, 4], y=[64], batch=[20801], ptr=[65])\n","\n","Step 58:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 97958], x=[21432, 4], y=[64], batch=[21432], ptr=[65])\n","\n","Step 59:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 114414], x=[25168, 4], y=[64], batch=[25168], ptr=[65])\n","\n","Step 60:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 94312], x=[20735, 4], y=[64], batch=[20735], ptr=[65])\n","\n","Step 61:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 113568], x=[24791, 4], y=[64], batch=[24791], ptr=[65])\n","\n","Step 62:\n","=======\n","Number of graphs in the current batch: 64\n","DataBatch(edge_index=[2, 109494], x=[24312, 4], y=[64], batch=[24312], ptr=[65])\n","\n","Step 63:\n","=======\n","Number of graphs in the current batch: 54\n","DataBatch(edge_index=[2, 110672], x=[23887, 4], y=[54], batch=[23887], ptr=[55])\n","\n"]}]},{"cell_type":"markdown","source":["### Build GCN & GNN Model"],"metadata":{"id":"zONj3ZV394EP"}},{"cell_type":"code","source":["class GCN(torch.nn.Module):\n","    def __init__(self, num_classes, hidden_dim, node_features_dim, edge_features_dim=None):\n","        super(GCN, self).__init__()\n","        torch.manual_seed(12345)\n","        self.conv1 = GCNConv(node_features_dim, hidden_dim)\n","        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n","        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n","        self.conv4 = GCNConv(hidden_dim, hidden_dim)\n","        self.conv5 = GCNConv(hidden_dim, hidden_dim)\n","        self.lin = Linear(hidden_dim, num_classes)\n","\n","    def forward(self, x, edge_index, batch):\n","        # 1. Obtain node embeddings \n","        x = self.conv1(x, edge_index)\n","        x = x.relu()\n","        x = self.conv2(x, edge_index)\n","        x = x.relu()\n","        x = self.conv3(x, edge_index)\n","        x = x.relu()\n","        x = self.conv4(x, edge_index)\n","        x = x.relu()\n","        x = self.conv5(x, edge_index)\n","        # 2. Readout layer\n","        x = global_mean_pool(x, batch)\n","        \n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.lin(x)\n","        \n","        return x\n","\n","\n","class GNN(torch.nn.Module):\n","    def __init__( self, num_classes, hidden_dim, node_features_dim, edge_features_dim=None ):\n","        super(GNN, self).__init__()\n","        self.hidden_dim = hidden_dim\n","\n","        self.conv1 = GraphConv(node_features_dim, hidden_dim)\n","        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n","        self.conv3 = GraphConv(hidden_dim, hidden_dim)\n","        self.conv4 = GraphConv(hidden_dim, hidden_dim)\n","        self.conv5 = GraphConv(hidden_dim, hidden_dim)\n","\n","        self.fc1 = Linear(hidden_dim, hidden_dim)\n","        self.fc2 = Linear(hidden_dim, num_classes)\n","\n","        self.readout = LogSoftmax(dim=-1)\n","\n","    def forward(self, x, edge_index, batch):\n","        x = F.relu(self.conv1(x, edge_index))\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = F.relu(self.conv2(x, edge_index))\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = F.relu(self.conv3(x, edge_index))\n","        x = F.relu(self.conv4(x, edge_index))\n","        x = F.relu(self.conv5(x, edge_index))\n","        x = global_add_pool(x, batch)\n","        x = F.relu(self.fc1(x))\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.fc2(x)\n","        \n","        return self.readout(x)"],"metadata":{"id":"TUiTodsoe2Un","executionInfo":{"status":"ok","timestamp":1652891008906,"user_tz":-420,"elapsed":18,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["print(f'Number of training graphs: {len(train_dataset)}')\n","print(f'Number of val graphs: {len(val_dataset)}')\n","print(f'Number of test graphs: {len(test_dataset)}')\n","print(\"**************************\")\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n","model = GNN(num_classes=dataset_train.num_classes, hidden_dim=64, node_features_dim=dataset_train.num_node_features).to(device)\n","\n","print('*****Model size is: ', get_model_size(model))\n","print(\"=====Model parameters are: \", count_parameters(model)) \n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qS2UCltieztt","executionInfo":{"status":"ok","timestamp":1652891020068,"user_tz":-420,"elapsed":11179,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}},"outputId":"afaa520b-c5cf-43b4-9d3e-93150bf821b0"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training graphs: 4022\n","Number of val graphs: 614\n","Number of test graphs: 994\n","**************************\n","*****Model size is:  157548\n","=====Model parameters are:  37955\n"]}]},{"cell_type":"code","source":["def plot_cm(cm, display_labels= ['BIRAD_0', 'BIRAD_1', 'BIRAD_2', 'BIRAD_3','BIRAD_4A', 'BIRAD_4B','BIRAD_4C', 'BIRAD_5']):\n"," \n","    # Set fontsize for plots\n","    font = {\"size\": 20}\n","    matplotlib.rc(\"font\", **font)\n","\n","    # Plot confusion matrix\n","    f, axes = plt.subplots(1, 1, figsize=(7, 7), sharey=\"row\")\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n","    disp.plot(ax=axes, xticks_rotation=45, cmap=\"Blues\", values_format='d')\n","    disp.im_.colorbar.remove()\n","    disp.ax_.set_xlabel(\"Predicted label\", fontsize=20)\n","    disp.ax_.set_ylabel(\"True label\", fontsize=20)\n","    matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n","    plt.show()"],"metadata":{"id":"1q-5quapf42v","executionInfo":{"status":"ok","timestamp":1652891020070,"user_tz":-420,"elapsed":22,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["train_losses, val_losses = [], [] \n","\n","def train():\n","    model.train() \n","    total_loss = 0.0\n","    for data in train_loader:  # Iterate in batches over the training dataset.\n","        data = data.to(device)\n","        label = data.y\n"," \n","        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass. \n","        loss = criterion(out, data.y)  # Compute the loss.\n","        total_loss = torch.sum(loss) + total_loss  \n","        loss.backward()  # Derive gradients. \n","        optimizer.step()  # Update parameters based on gradients.\n","        optimizer.zero_grad()  # Clear gradients.\n","    print('Train loss:', (float)(total_loss))\n","    train_losses.append((float)(total_loss))\n","    # return total_loss \n","\n","#@timeit()\n","def test(loader, is_up = True):\n","    model.eval()\n","    correct = 0\n","    y_pred = []\n","    y_true = []\n","    total_loss = 0.0\n","    for data in loader:   \n","        data = data.to(device)\n","        out = model(data.x, data.edge_index, data.batch)  \n","\n","        loss = criterion(out, data.y)\n","        total_loss = torch.sum(loss) + total_loss \n","\n","        pred = out.argmax(dim=1)  \n","        correct += int((pred == data.y).sum())   \n","\n","        y_true.extend(data.y.cpu().numpy())\n","        y_pred.extend(np.squeeze(pred.cpu().numpy().T))\n","    # report = classification_report(y_true, y_pred, digits=4) \n","    if is_up == True: \n","        print('Val loss:', (float)(total_loss))\n","        val_losses.append((float)(total_loss))\n","    return correct / len(loader.dataset) # , total_loss\n","\n","\n","def inference(loader):\n","    model.eval()\n","    correct = 0\n","    y_pred = []\n","    y_true = []\n","    for data in tqdm.tqdm(loader):\n","        data = data.to('cpu')\n","        out = model(data.x, data.edge_index, data.batch)  \n","        pred = out.argmax(dim=1)  \n","        print('==============')\n","        y_true.extend(data.y.cpu().numpy())\n","        y_pred.extend(np.squeeze(pred.cpu().numpy().T))\n","    report = classification_report(y_true, y_pred, digits=4)\n","    cm = confusion_matrix(y_true, y_pred)\n","    display_labels = ['Bacteria', 'Normal', 'Virus']\n","    plot_cm(cm=cm, display_labels=display_labels)    \n","\n","    return correct / len(loader.dataset)"],"metadata":{"id":"zFVkDzTMfoh2","executionInfo":{"status":"ok","timestamp":1652891020072,"user_tz":-420,"elapsed":22,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["num_epochs = 500\n","start = time.time()\n","best_val_acc = 0.9\n","train_accs, val_accs = [], []\n","train_losses, val_losses = [], [] \n","\n","for epoch in tqdm.tqdm(range(0, num_epochs)):\n","    train()\n","    train_acc = test(train_loader, is_up = False)\n","    val_acc = test(val_loader, is_up = True)\n","\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        save_weight_path = osp.join(f\"GNN_\" + name_dataset + \"_best\" + \".pth\")\n","        print('New best model saved to:', save_weight_path)\n","        torch.save(model.state_dict(), save_weight_path)\n","\n","    # if epoch % 10 == 0:\n","    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Validation Acc: {val_acc:.4f}')\n","    \n","    train_accs.append(train_acc)\n","    # train_losses.append(train_loss) \n","\n","    val_accs.append(val_acc)\n","    # val_losses.append(val_loss)"],"metadata":{"id":"HYv3QLdmfq2F","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["992fd95bc77440b7954fa0e430dd277b","f9d09e1e6fea4fce926e740bbaf6f160","691b628693894ca9acd1582dff7615bf","d759a38b1c834feea1280598625cea2f","c5a48bd84add4d3c875b7739e5c9f312","982c7d6eeeba411ca0c2e180fbb4407e","8ee99c6113b54b76942b077e7ef49664","dc9f492d221a40cf829d8165661d4f85","d8b43f832fd542739b84131c4e1954c2","7e141c14a42c4ea8a24196226b137452","68bdcca9b1bd4477a70fd32b4c929838"]},"outputId":"e2dcc06d-4510-4d07-86e7-11a6adbe82f3","executionInfo":{"status":"ok","timestamp":1652892426651,"user_tz":-420,"elapsed":752711,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}}},"execution_count":14,"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"992fd95bc77440b7954fa0e430dd277b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/500 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"output_type":"stream","name":"stdout","text":["Train loss: 26745.9609375\n","Val loss: 660.9125366210938\n","New best model saved to: GNN_CXR_Reformat_128_CXR_Prewitt_Testv2_best.pth\n","Epoch: 000, Train Acc: 0.9709, Validation Acc: 0.9707\n","Train loss: 4260.84912109375\n","Val loss: 516.7020263671875\n","Epoch: 001, Train Acc: 0.9739, Validation Acc: 0.9707\n","Train loss: 2661.66259765625\n","Val loss: 454.8880310058594\n","Epoch: 002, Train Acc: 0.9741, Validation Acc: 0.9707\n","Train loss: 1900.03759765625\n","Val loss: 403.0007629394531\n","New best model saved to: GNN_CXR_Reformat_128_CXR_Prewitt_Testv2_best.pth\n","Epoch: 003, Train Acc: 0.9739, Validation Acc: 0.9723\n","Train loss: 1293.9578857421875\n","Val loss: 370.2259216308594\n","Epoch: 004, Train Acc: 0.9739, Validation Acc: 0.9723\n","Train loss: 980.3457641601562\n","Val loss: 347.3230285644531\n","Epoch: 005, Train Acc: 0.9739, Validation Acc: 0.9707\n","Train loss: 624.36279296875\n","Val loss: 305.7716064453125\n","Epoch: 006, Train Acc: 0.9739, Validation Acc: 0.9691\n","Train loss: 477.5736999511719\n","Val loss: 324.3023376464844\n","Epoch: 007, Train Acc: 0.9679, Validation Acc: 0.9658\n","Train loss: 320.51788330078125\n","Val loss: 305.1994934082031\n","Epoch: 008, Train Acc: 0.9699, Validation Acc: 0.9642\n","Train loss: 217.7150115966797\n","Val loss: 310.1129455566406\n","Epoch: 009, Train Acc: 0.9699, Validation Acc: 0.9674\n","Train loss: 181.69700622558594\n","Val loss: 319.08160400390625\n","Epoch: 010, Train Acc: 0.9674, Validation Acc: 0.9658\n","Train loss: 165.90663146972656\n","Val loss: 292.1357421875\n","Epoch: 011, Train Acc: 0.9674, Validation Acc: 0.9642\n","Train loss: 79.41962432861328\n","Val loss: 345.5818176269531\n","Epoch: 012, Train Acc: 0.9615, Validation Acc: 0.9560\n","Train loss: 57.76284408569336\n","Val loss: 362.4306640625\n","Epoch: 013, Train Acc: 0.9689, Validation Acc: 0.9674\n","Train loss: 56.6057014465332\n","Val loss: 295.3589172363281\n","Epoch: 014, Train Acc: 0.9664, Validation Acc: 0.9658\n","Train loss: 42.00975799560547\n","Val loss: 301.8414001464844\n","Epoch: 015, Train Acc: 0.9642, Validation Acc: 0.9609\n","Train loss: 23.255964279174805\n","Val loss: 304.9920349121094\n","Epoch: 016, Train Acc: 0.9530, Validation Acc: 0.9528\n","Train loss: 22.36056137084961\n","Val loss: 299.0663757324219\n","Epoch: 017, Train Acc: 0.9550, Validation Acc: 0.9479\n","Train loss: 26.709260940551758\n","Val loss: 296.0998229980469\n","Epoch: 018, Train Acc: 0.9615, Validation Acc: 0.9560\n","Train loss: 18.403564453125\n","Val loss: 305.1444091796875\n","Epoch: 019, Train Acc: 0.9359, Validation Acc: 0.9186\n","Train loss: 20.680160522460938\n","Val loss: 316.06170654296875\n","Epoch: 020, Train Acc: 0.8483, Validation Acc: 0.8225\n","Train loss: 13.788268089294434\n","Val loss: 302.4713134765625\n","Epoch: 021, Train Acc: 0.8531, Validation Acc: 0.8404\n","Train loss: 26.315088272094727\n","Val loss: 285.63641357421875\n","Epoch: 022, Train Acc: 0.8680, Validation Acc: 0.8599\n","Train loss: 16.037010192871094\n","Val loss: 286.564697265625\n","Epoch: 023, Train Acc: 0.8446, Validation Acc: 0.8274\n","Train loss: 13.723410606384277\n","Val loss: 286.8572082519531\n","Epoch: 024, Train Acc: 0.8729, Validation Acc: 0.8518\n","Train loss: 13.403890609741211\n","Val loss: 350.71728515625\n","Epoch: 025, Train Acc: 0.8782, Validation Acc: 0.8616\n","Train loss: 14.230494499206543\n","Val loss: 287.5487976074219\n","Epoch: 026, Train Acc: 0.8779, Validation Acc: 0.8648\n","Train loss: 12.616652488708496\n","Val loss: 287.2823791503906\n","Epoch: 027, Train Acc: 0.9068, Validation Acc: 0.8990\n","Train loss: 12.953502655029297\n","Val loss: 298.6003112792969\n","Epoch: 028, Train Acc: 0.8424, Validation Acc: 0.8208\n","Train loss: 11.969417572021484\n","Val loss: 291.8998107910156\n","Epoch: 029, Train Acc: 0.8337, Validation Acc: 0.8160\n","Train loss: 11.920855522155762\n","Val loss: 281.1280822753906\n","Epoch: 030, Train Acc: 0.8757, Validation Acc: 0.8664\n","Train loss: 11.876898765563965\n","Val loss: 328.7937927246094\n","Epoch: 031, Train Acc: 0.8680, Validation Acc: 0.8681\n","Train loss: 12.463939666748047\n","Val loss: 294.3521728515625\n","Epoch: 032, Train Acc: 0.8981, Validation Acc: 0.9072\n","Train loss: 11.539228439331055\n","Val loss: 296.1820983886719\n","Epoch: 033, Train Acc: 0.8774, Validation Acc: 0.8811\n","Train loss: 12.310253143310547\n","Val loss: 281.4567565917969\n","Epoch: 034, Train Acc: 0.8896, Validation Acc: 0.9039\n","Train loss: 10.899442672729492\n","Val loss: 331.7198791503906\n","Epoch: 035, Train Acc: 0.8856, Validation Acc: 0.9023\n","Train loss: 11.499069213867188\n","Val loss: 280.7315673828125\n","Epoch: 036, Train Acc: 0.8946, Validation Acc: 0.9072\n","Train loss: 11.812355995178223\n","Val loss: 283.5914001464844\n","Epoch: 037, Train Acc: 0.8971, Validation Acc: 0.9055\n","Train loss: 11.055170059204102\n","Val loss: 281.52850341796875\n","Epoch: 038, Train Acc: 0.9132, Validation Acc: 0.9300\n","Train loss: 13.0247220993042\n","Val loss: 332.52008056640625\n","Epoch: 039, Train Acc: 0.9334, Validation Acc: 0.9430\n","Train loss: 12.982739448547363\n","Val loss: 306.86737060546875\n","Epoch: 040, Train Acc: 0.9344, Validation Acc: 0.9446\n","Train loss: 12.623580932617188\n","Val loss: 304.8620300292969\n","Epoch: 041, Train Acc: 0.9351, Validation Acc: 0.9430\n","Train loss: 11.282968521118164\n","Val loss: 298.528564453125\n","Epoch: 042, Train Acc: 0.9582, Validation Acc: 0.9658\n","Train loss: 10.53663158416748\n","Val loss: 303.17315673828125\n","Epoch: 043, Train Acc: 0.9617, Validation Acc: 0.9625\n","Train loss: 10.274467468261719\n","Val loss: 348.14141845703125\n","Epoch: 044, Train Acc: 0.9515, Validation Acc: 0.9625\n","Train loss: 10.133371353149414\n","Val loss: 348.5089416503906\n","Epoch: 045, Train Acc: 0.9525, Validation Acc: 0.9593\n","Train loss: 11.424302101135254\n","Val loss: 301.0387268066406\n","Epoch: 046, Train Acc: 0.9707, Validation Acc: 0.9658\n","Train loss: 11.066484451293945\n","Val loss: 342.4584655761719\n","Epoch: 047, Train Acc: 0.9702, Validation Acc: 0.9658\n","Train loss: 13.918051719665527\n","Val loss: 375.135986328125\n","Epoch: 048, Train Acc: 0.9684, Validation Acc: 0.9658\n","Train loss: 9.755294799804688\n","Val loss: 315.1614074707031\n","Epoch: 049, Train Acc: 0.9679, Validation Acc: 0.9658\n","Train loss: 11.390628814697266\n","Val loss: 297.37261962890625\n","Epoch: 050, Train Acc: 0.9692, Validation Acc: 0.9658\n","Train loss: 9.814061164855957\n","Val loss: 295.02655029296875\n","Epoch: 051, Train Acc: 0.9585, Validation Acc: 0.9658\n","Train loss: 10.040131568908691\n","Val loss: 299.16485595703125\n","Epoch: 052, Train Acc: 0.9602, Validation Acc: 0.9609\n","Train loss: 10.374249458312988\n","Val loss: 300.691650390625\n","Epoch: 053, Train Acc: 0.9600, Validation Acc: 0.9593\n","Train loss: 10.241103172302246\n","Val loss: 295.72650146484375\n","Epoch: 054, Train Acc: 0.9697, Validation Acc: 0.9658\n","Train loss: 10.021900177001953\n","Val loss: 298.18646240234375\n","Epoch: 055, Train Acc: 0.9264, Validation Acc: 0.9153\n","Train loss: 10.507002830505371\n","Val loss: 354.58587646484375\n","Epoch: 056, Train Acc: 0.9687, Validation Acc: 0.9625\n","Train loss: 9.855745315551758\n","Val loss: 305.2722473144531\n","Epoch: 057, Train Acc: 0.9689, Validation Acc: 0.9625\n","Train loss: 10.032814979553223\n","Val loss: 410.88787841796875\n","Epoch: 058, Train Acc: 0.9694, Validation Acc: 0.9625\n","Train loss: 12.328248023986816\n","Val loss: 310.5780334472656\n","Epoch: 059, Train Acc: 0.9684, Validation Acc: 0.9658\n","Train loss: 9.745230674743652\n","Val loss: 301.30047607421875\n","Epoch: 060, Train Acc: 0.9667, Validation Acc: 0.9642\n","Train loss: 9.57168197631836\n","Val loss: 283.9664306640625\n","Epoch: 061, Train Acc: 0.9659, Validation Acc: 0.9642\n","Train loss: 9.246232032775879\n","Val loss: 283.9378662109375\n","Epoch: 062, Train Acc: 0.9657, Validation Acc: 0.9609\n","Train loss: 9.165617942810059\n","Val loss: 286.44580078125\n","Epoch: 063, Train Acc: 0.9649, Validation Acc: 0.9625\n","Train loss: 9.443720817565918\n","Val loss: 289.46307373046875\n","Epoch: 064, Train Acc: 0.9647, Validation Acc: 0.9593\n","Train loss: 8.725404739379883\n","Val loss: 355.544189453125\n","Epoch: 065, Train Acc: 0.9632, Validation Acc: 0.9593\n","Train loss: 9.148048400878906\n","Val loss: 290.427734375\n","Epoch: 066, Train Acc: 0.9652, Validation Acc: 0.9625\n","Train loss: 11.335662841796875\n","Val loss: 238.41383361816406\n","Epoch: 067, Train Acc: 0.9704, Validation Acc: 0.9625\n","Train loss: 9.656485557556152\n","Val loss: 244.2801971435547\n","Epoch: 068, Train Acc: 0.9704, Validation Acc: 0.9625\n","Train loss: 10.491825103759766\n","Val loss: 249.04263305664062\n","Epoch: 069, Train Acc: 0.9694, Validation Acc: 0.9642\n","Train loss: 8.553786277770996\n","Val loss: 249.8215789794922\n","Epoch: 070, Train Acc: 0.9689, Validation Acc: 0.9642\n","Train loss: 9.11798095703125\n","Val loss: 248.642578125\n","Epoch: 071, Train Acc: 0.9687, Validation Acc: 0.9625\n","Train loss: 9.480308532714844\n","Val loss: 291.2464904785156\n","Epoch: 072, Train Acc: 0.9689, Validation Acc: 0.9609\n","Train loss: 9.020854949951172\n","Val loss: 251.06004333496094\n","Epoch: 073, Train Acc: 0.9687, Validation Acc: 0.9609\n","Train loss: 8.56604290008545\n","Val loss: 254.09339904785156\n","Epoch: 074, Train Acc: 0.9694, Validation Acc: 0.9642\n","Train loss: 8.781920433044434\n","Val loss: 257.9763488769531\n","Epoch: 075, Train Acc: 0.9694, Validation Acc: 0.9642\n","Train loss: 8.862713813781738\n","Val loss: 261.8548889160156\n","Epoch: 076, Train Acc: 0.9694, Validation Acc: 0.9625\n","Train loss: 8.506743431091309\n","Val loss: 262.9699401855469\n","Epoch: 077, Train Acc: 0.9692, Validation Acc: 0.9642\n","Train loss: 8.641844749450684\n","Val loss: 262.9338073730469\n","Epoch: 078, Train Acc: 0.9687, Validation Acc: 0.9625\n","Train loss: 8.2556791305542\n","Val loss: 266.2635803222656\n","Epoch: 079, Train Acc: 0.9689, Validation Acc: 0.9625\n","Train loss: 8.517890930175781\n","Val loss: 267.7279357910156\n","Epoch: 080, Train Acc: 0.9692, Validation Acc: 0.9625\n","Train loss: 8.513832092285156\n","Val loss: 272.7896423339844\n","Epoch: 081, Train Acc: 0.9694, Validation Acc: 0.9642\n","Train loss: 8.500876426696777\n","Val loss: 275.4664306640625\n","Epoch: 082, Train Acc: 0.9687, Validation Acc: 0.9625\n","Train loss: 8.703367233276367\n","Val loss: 317.7938232421875\n","Epoch: 083, Train Acc: 0.9684, Validation Acc: 0.9625\n","Train loss: 8.650773048400879\n","Val loss: 352.2359619140625\n","Epoch: 084, Train Acc: 0.9697, Validation Acc: 0.9642\n","Train loss: 10.06687068939209\n","Val loss: 255.7036895751953\n","Epoch: 085, Train Acc: 0.9699, Validation Acc: 0.9642\n","Train loss: 8.024120330810547\n","Val loss: 258.8467102050781\n","Epoch: 086, Train Acc: 0.9694, Validation Acc: 0.9658\n","Train loss: 8.814126014709473\n","Val loss: 263.9148864746094\n","Epoch: 087, Train Acc: 0.9702, Validation Acc: 0.9658\n","Train loss: 8.334653854370117\n","Val loss: 278.2952575683594\n","Epoch: 088, Train Acc: 0.9707, Validation Acc: 0.9658\n","Train loss: 8.962244033813477\n","Val loss: 271.1930847167969\n","Epoch: 089, Train Acc: 0.9712, Validation Acc: 0.9642\n","Train loss: 8.010629653930664\n","Val loss: 279.5157775878906\n","Epoch: 090, Train Acc: 0.9717, Validation Acc: 0.9658\n","Train loss: 8.394826889038086\n","Val loss: 266.45062255859375\n","Epoch: 091, Train Acc: 0.9709, Validation Acc: 0.9658\n","Train loss: 8.19405460357666\n","Val loss: 291.0405578613281\n","Epoch: 092, Train Acc: 0.9707, Validation Acc: 0.9658\n","Train loss: 7.90833854675293\n","Val loss: 320.10577392578125\n","Epoch: 093, Train Acc: 0.9709, Validation Acc: 0.9658\n","Train loss: 8.179108619689941\n","Val loss: 302.7632751464844\n","Epoch: 094, Train Acc: 0.9707, Validation Acc: 0.9658\n","Train loss: 8.033506393432617\n","Val loss: 267.0023193359375\n","Epoch: 095, Train Acc: 0.9712, Validation Acc: 0.9658\n","Train loss: 8.316191673278809\n","Val loss: 274.0102233886719\n","Epoch: 096, Train Acc: 0.9709, Validation Acc: 0.9658\n","Train loss: 7.8195600509643555\n","Val loss: 278.655517578125\n","Epoch: 097, Train Acc: 0.9709, Validation Acc: 0.9642\n","Train loss: 8.140425682067871\n","Val loss: 322.03826904296875\n","Epoch: 098, Train Acc: 0.9709, Validation Acc: 0.9642\n","Train loss: 8.126569747924805\n","Val loss: 300.4105529785156\n","Epoch: 099, Train Acc: 0.9709, Validation Acc: 0.9625\n","Train loss: 8.067700386047363\n","Val loss: 287.1369323730469\n","Epoch: 100, Train Acc: 0.9712, Validation Acc: 0.9625\n","Train loss: 7.486990451812744\n","Val loss: 350.42437744140625\n","Epoch: 101, Train Acc: 0.9712, Validation Acc: 0.9625\n","Train loss: 7.761427402496338\n","Val loss: 302.42352294921875\n","Epoch: 102, Train Acc: 0.9712, Validation Acc: 0.9625\n","Train loss: 7.572740077972412\n","Val loss: 300.03582763671875\n","Epoch: 103, Train Acc: 0.9712, Validation Acc: 0.9609\n","Train loss: 7.717136859893799\n","Val loss: 319.5537109375\n","Epoch: 104, Train Acc: 0.9714, Validation Acc: 0.9609\n","Train loss: 7.719760417938232\n","Val loss: 304.5946960449219\n","Epoch: 105, Train Acc: 0.9717, Validation Acc: 0.9609\n","Train loss: 7.9447760581970215\n","Val loss: 396.53204345703125\n","Epoch: 106, Train Acc: 0.9714, Validation Acc: 0.9609\n","Train loss: 7.74701452255249\n","Val loss: 357.0990905761719\n","Epoch: 107, Train Acc: 0.9714, Validation Acc: 0.9609\n","Train loss: 7.768251419067383\n","Val loss: 308.59564208984375\n","Epoch: 108, Train Acc: 0.9722, Validation Acc: 0.9609\n","Train loss: 7.4550323486328125\n","Val loss: 314.8035888671875\n","Epoch: 109, Train Acc: 0.9722, Validation Acc: 0.9625\n","Train loss: 7.340917587280273\n","Val loss: 321.41802978515625\n","Epoch: 110, Train Acc: 0.9719, Validation Acc: 0.9625\n","Train loss: 8.09188175201416\n","Val loss: 327.06207275390625\n","Epoch: 111, Train Acc: 0.9722, Validation Acc: 0.9658\n","Train loss: 8.507307052612305\n","Val loss: 333.936279296875\n","Epoch: 112, Train Acc: 0.9719, Validation Acc: 0.9642\n","Train loss: 7.403536319732666\n","Val loss: 353.2453308105469\n","Epoch: 113, Train Acc: 0.9719, Validation Acc: 0.9609\n","Train loss: 7.537789344787598\n","Val loss: 385.1011047363281\n","Epoch: 114, Train Acc: 0.9719, Validation Acc: 0.9609\n","Train loss: 7.585190773010254\n","Val loss: 353.02960205078125\n","Epoch: 115, Train Acc: 0.9722, Validation Acc: 0.9609\n","Train loss: 7.843311309814453\n","Val loss: 370.9439392089844\n","Epoch: 116, Train Acc: 0.9722, Validation Acc: 0.9609\n","Train loss: 7.319466590881348\n","Val loss: 353.6406555175781\n","Epoch: 117, Train Acc: 0.9722, Validation Acc: 0.9609\n","Train loss: 8.154150009155273\n","Val loss: 374.7071838378906\n","Epoch: 118, Train Acc: 0.9719, Validation Acc: 0.9609\n","Train loss: 8.246682167053223\n","Val loss: 359.2950134277344\n","Epoch: 119, Train Acc: 0.9722, Validation Acc: 0.9625\n","Train loss: 7.495412826538086\n","Val loss: 368.6587829589844\n","Epoch: 120, Train Acc: 0.9722, Validation Acc: 0.9609\n","Train loss: 7.358823776245117\n","Val loss: 367.9853515625\n","Epoch: 121, Train Acc: 0.9719, Validation Acc: 0.9609\n","Train loss: 7.453298568725586\n","Val loss: 367.6466369628906\n","Epoch: 122, Train Acc: 0.9722, Validation Acc: 0.9609\n","Train loss: 7.02710485458374\n","Val loss: 347.150390625\n","Epoch: 123, Train Acc: 0.9719, Validation Acc: 0.9609\n","Train loss: 7.197238445281982\n","Val loss: 350.2715148925781\n","Epoch: 124, Train Acc: 0.9719, Validation Acc: 0.9609\n","Train loss: 7.168181896209717\n","Val loss: 349.3343200683594\n","Epoch: 125, Train Acc: 0.9722, Validation Acc: 0.9609\n","Train loss: 7.36548376083374\n","Val loss: 359.7692565917969\n","Epoch: 126, Train Acc: 0.9722, Validation Acc: 0.9609\n","Train loss: 6.950474262237549\n","Val loss: 361.4680480957031\n","Epoch: 127, Train Acc: 0.9722, Validation Acc: 0.9609\n","Train loss: 7.182266712188721\n","Val loss: 360.56390380859375\n","Epoch: 128, Train Acc: 0.9722, Validation Acc: 0.9609\n","Train loss: 7.077373504638672\n","Val loss: 364.62274169921875\n","Epoch: 129, Train Acc: 0.9724, Validation Acc: 0.9609\n","Train loss: 7.20494270324707\n","Val loss: 389.4179382324219\n","Epoch: 130, Train Acc: 0.9722, Validation Acc: 0.9609\n","Train loss: 18.023597717285156\n","Val loss: 218.8712615966797\n","Epoch: 131, Train Acc: 0.9677, Validation Acc: 0.9511\n","Train loss: 15.816549301147461\n","Val loss: 691.8228759765625\n","Epoch: 132, Train Acc: 0.9707, Validation Acc: 0.9642\n","Train loss: 7.533098220825195\n","Val loss: 628.783935546875\n","Epoch: 133, Train Acc: 0.9719, Validation Acc: 0.9642\n","Train loss: 6.979141712188721\n","Val loss: 631.2822265625\n","Epoch: 134, Train Acc: 0.9719, Validation Acc: 0.9642\n","Train loss: 7.237344741821289\n","Val loss: 669.2667236328125\n","Epoch: 135, Train Acc: 0.9719, Validation Acc: 0.9642\n","Train loss: 6.85399866104126\n","Val loss: 637.1881713867188\n","Epoch: 136, Train Acc: 0.9719, Validation Acc: 0.9642\n","Train loss: 7.183920860290527\n","Val loss: 634.37353515625\n","Epoch: 137, Train Acc: 0.9722, Validation Acc: 0.9642\n","Train loss: 7.0915985107421875\n","Val loss: 699.1676025390625\n","Epoch: 138, Train Acc: 0.9722, Validation Acc: 0.9642\n","Train loss: 6.953119277954102\n","Val loss: 667.6068725585938\n","Epoch: 139, Train Acc: 0.9724, Validation Acc: 0.9642\n","Train loss: 7.206900596618652\n","Val loss: 663.8203735351562\n","Epoch: 140, Train Acc: 0.9722, Validation Acc: 0.9642\n","Train loss: 6.9779052734375\n","Val loss: 783.1177978515625\n","Epoch: 141, Train Acc: 0.9722, Validation Acc: 0.9642\n","Train loss: 6.980799198150635\n","Val loss: 673.732421875\n","Epoch: 142, Train Acc: 0.9722, Validation Acc: 0.9642\n","Train loss: 6.65410041809082\n","Val loss: 675.2627563476562\n","Epoch: 143, Train Acc: 0.9722, Validation Acc: 0.9642\n","Train loss: 6.930095672607422\n","Val loss: 668.6861572265625\n","Epoch: 144, Train Acc: 0.9724, Validation Acc: 0.9658\n","Train loss: 6.589076042175293\n","Val loss: 662.8234252929688\n","Epoch: 145, Train Acc: 0.9724, Validation Acc: 0.9658\n","Train loss: 8.703629493713379\n","Val loss: 735.138671875\n","Epoch: 146, Train Acc: 0.9722, Validation Acc: 0.9658\n","Train loss: 7.239306926727295\n","Val loss: 676.9534301757812\n","Epoch: 147, Train Acc: 0.9724, Validation Acc: 0.9658\n","Train loss: 6.994627952575684\n","Val loss: 701.6472778320312\n","Epoch: 148, Train Acc: 0.9722, Validation Acc: 0.9658\n","Train loss: 6.979141712188721\n","Val loss: 659.1781005859375\n","Epoch: 149, Train Acc: 0.9722, Validation Acc: 0.9658\n","Train loss: 6.926133632659912\n","Val loss: 667.6942749023438\n","Epoch: 150, Train Acc: 0.9722, Validation Acc: 0.9658\n","Train loss: 7.389991760253906\n","Val loss: 657.01416015625\n","Epoch: 151, Train Acc: 0.9722, Validation Acc: 0.9658\n","Train loss: 6.8510966300964355\n","Val loss: 663.6520385742188\n","Epoch: 152, Train Acc: 0.9722, Validation Acc: 0.9658\n","Train loss: 6.840684413909912\n","Val loss: 670.2793579101562\n","Epoch: 153, Train Acc: 0.9724, Validation Acc: 0.9658\n","Train loss: 6.534622669219971\n","Val loss: 867.8076171875\n","Epoch: 154, Train Acc: 0.9722, Validation Acc: 0.9658\n","Train loss: 6.764252185821533\n","Val loss: 705.2733154296875\n","Epoch: 155, Train Acc: 0.9722, Validation Acc: 0.9658\n","Train loss: 6.720756530761719\n","Val loss: 683.0154418945312\n","Epoch: 156, Train Acc: 0.9722, Validation Acc: 0.9642\n","Train loss: 6.4407196044921875\n","Val loss: 685.6539916992188\n","Epoch: 157, Train Acc: 0.9722, Validation Acc: 0.9642\n","Train loss: 6.499542713165283\n","Val loss: 690.2734375\n","Epoch: 158, Train Acc: 0.9722, Validation Acc: 0.9642\n","Train loss: 6.5506696701049805\n","Val loss: 691.1959838867188\n","Epoch: 159, Train Acc: 0.9722, Validation Acc: 0.9642\n","Train loss: 6.612680435180664\n","Val loss: 700.3091430664062\n","Epoch: 160, Train Acc: 0.9722, Validation Acc: 0.9642\n","Train loss: 6.644730091094971\n","Val loss: 738.1588134765625\n","Epoch: 161, Train Acc: 0.9722, Validation Acc: 0.9642\n","Train loss: 6.655066013336182\n","Val loss: 699.1064453125\n","Epoch: 162, Train Acc: 0.9722, Validation Acc: 0.9642\n","Train loss: 6.357009410858154\n","Val loss: 705.8331298828125\n","Epoch: 163, Train Acc: 0.9727, Validation Acc: 0.9642\n","Train loss: 6.342177867889404\n","Val loss: 700.7345581054688\n","Epoch: 164, Train Acc: 0.9727, Validation Acc: 0.9642\n","Train loss: 6.451056003570557\n","Val loss: 823.8521728515625\n","Epoch: 165, Train Acc: 0.9727, Validation Acc: 0.9642\n","Train loss: 6.605127334594727\n","Val loss: 697.8711547851562\n","Epoch: 166, Train Acc: 0.9727, Validation Acc: 0.9642\n","Train loss: 6.508824825286865\n","Val loss: 696.9663696289062\n","Epoch: 167, Train Acc: 0.9727, Validation Acc: 0.9674\n","Train loss: 6.463628768920898\n","Val loss: 686.316650390625\n","Epoch: 168, Train Acc: 0.9731, Validation Acc: 0.9674\n","Train loss: 6.677018165588379\n","Val loss: 664.9749145507812\n","Epoch: 169, Train Acc: 0.9734, Validation Acc: 0.9642\n","Train loss: 6.791670322418213\n","Val loss: 820.3756103515625\n","Epoch: 170, Train Acc: 0.9734, Validation Acc: 0.9642\n","Train loss: 7.096391201019287\n","Val loss: 670.9539794921875\n","Epoch: 171, Train Acc: 0.9734, Validation Acc: 0.9642\n","Train loss: 6.841106414794922\n","Val loss: 857.12158203125\n","Epoch: 172, Train Acc: 0.9734, Validation Acc: 0.9642\n","Train loss: 6.963905334472656\n","Val loss: 694.5158081054688\n","Epoch: 173, Train Acc: 0.9744, Validation Acc: 0.9642\n","Train loss: 6.795206069946289\n","Val loss: 700.8077392578125\n","Epoch: 174, Train Acc: 0.9744, Validation Acc: 0.9642\n","Train loss: 6.836000442504883\n","Val loss: 725.69970703125\n","Epoch: 175, Train Acc: 0.9734, Validation Acc: 0.9642\n","Train loss: 6.6806464195251465\n","Val loss: 672.0775756835938\n","Epoch: 176, Train Acc: 0.9734, Validation Acc: 0.9642\n","Train loss: 6.845958709716797\n","Val loss: 676.7022705078125\n","Epoch: 177, Train Acc: 0.9734, Validation Acc: 0.9642\n","Train loss: 6.846567630767822\n","Val loss: 687.49560546875\n","Epoch: 178, Train Acc: 0.9744, Validation Acc: 0.9642\n","Train loss: 6.708243370056152\n","Val loss: 749.5316162109375\n","Epoch: 179, Train Acc: 0.9744, Validation Acc: 0.9642\n","Train loss: 6.713286399841309\n","Val loss: 713.4708251953125\n","Epoch: 180, Train Acc: 0.9744, Validation Acc: 0.9674\n","Train loss: 6.458972930908203\n","Val loss: 798.5076904296875\n","Epoch: 181, Train Acc: 0.9744, Validation Acc: 0.9674\n","Train loss: 6.340682029724121\n","Val loss: 849.076416015625\n","Epoch: 182, Train Acc: 0.9744, Validation Acc: 0.9674\n","Train loss: 6.132683277130127\n","Val loss: 764.0910034179688\n","Epoch: 183, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.418070316314697\n","Val loss: 784.6154174804688\n","Epoch: 184, Train Acc: 0.9744, Validation Acc: 0.9674\n","Train loss: 6.2512359619140625\n","Val loss: 814.7597045898438\n","Epoch: 185, Train Acc: 0.9744, Validation Acc: 0.9674\n","Train loss: 6.350492477416992\n","Val loss: 864.2825317382812\n","Epoch: 186, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 7.061882019042969\n","Val loss: 909.6494140625\n","Epoch: 187, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.405966758728027\n","Val loss: 701.4832153320312\n","Epoch: 188, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.273232936859131\n","Val loss: 879.7796020507812\n","Epoch: 189, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.090698719024658\n","Val loss: 731.13330078125\n","Epoch: 190, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.1495466232299805\n","Val loss: 751.3772583007812\n","Epoch: 191, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.225568771362305\n","Val loss: 788.242431640625\n","Epoch: 192, Train Acc: 0.9744, Validation Acc: 0.9674\n","Train loss: 6.269773960113525\n","Val loss: 876.2894287109375\n","Epoch: 193, Train Acc: 0.9744, Validation Acc: 0.9674\n","Train loss: 6.806335926055908\n","Val loss: 805.7919311523438\n","Epoch: 194, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.307709693908691\n","Val loss: 749.63525390625\n","Epoch: 195, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.321626663208008\n","Val loss: 845.6832275390625\n","Epoch: 196, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.017899036407471\n","Val loss: 990.1207275390625\n","Epoch: 197, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.199193000793457\n","Val loss: 958.7978515625\n","Epoch: 198, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 7.000038146972656\n","Val loss: 905.8574829101562\n","Epoch: 199, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.9932451248168945\n","Val loss: 971.480224609375\n","Epoch: 200, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.740139484405518\n","Val loss: 870.54833984375\n","Epoch: 201, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.232085704803467\n","Val loss: 902.342529296875\n","Epoch: 202, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.304470062255859\n","Val loss: 973.7049560546875\n","Epoch: 203, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.033566474914551\n","Val loss: 909.6906127929688\n","Epoch: 204, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.527647972106934\n","Val loss: 886.5707397460938\n","Epoch: 205, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.071479797363281\n","Val loss: 897.0757446289062\n","Epoch: 206, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.309773921966553\n","Val loss: 903.4295043945312\n","Epoch: 207, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.056937217712402\n","Val loss: 998.979248046875\n","Epoch: 208, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.229587554931641\n","Val loss: 843.5887451171875\n","Epoch: 209, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.003345012664795\n","Val loss: 1070.42236328125\n","Epoch: 210, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.184157848358154\n","Val loss: 849.3079223632812\n","Epoch: 211, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.276783466339111\n","Val loss: 917.02001953125\n","Epoch: 212, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.037020206451416\n","Val loss: 882.6168212890625\n","Epoch: 213, Train Acc: 0.9744, Validation Acc: 0.9674\n","Train loss: 6.192720413208008\n","Val loss: 968.937255859375\n","Epoch: 214, Train Acc: 0.9744, Validation Acc: 0.9674\n","Train loss: 6.126056671142578\n","Val loss: 949.4232788085938\n","Epoch: 215, Train Acc: 0.9744, Validation Acc: 0.9674\n","Train loss: 6.395094871520996\n","Val loss: 1091.7132568359375\n","Epoch: 216, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 6.282619476318359\n","Val loss: 1011.0349731445312\n","Epoch: 217, Train Acc: 0.9744, Validation Acc: 0.9674\n","Train loss: 10.78919792175293\n","Val loss: 597.6415405273438\n","Epoch: 218, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.418344974517822\n","Val loss: 672.0939331054688\n","Epoch: 219, Train Acc: 0.9744, Validation Acc: 0.9642\n","Train loss: 6.269080638885498\n","Val loss: 846.824951171875\n","Epoch: 220, Train Acc: 0.9744, Validation Acc: 0.9642\n","Train loss: 5.914449691772461\n","Val loss: 701.3418579101562\n","Epoch: 221, Train Acc: 0.9744, Validation Acc: 0.9642\n","Train loss: 6.364140510559082\n","Val loss: 761.9962158203125\n","Epoch: 222, Train Acc: 0.9744, Validation Acc: 0.9642\n","Train loss: 6.088985443115234\n","Val loss: 782.3265380859375\n","Epoch: 223, Train Acc: 0.9744, Validation Acc: 0.9642\n","Train loss: 6.109817981719971\n","Val loss: 837.8944702148438\n","Epoch: 224, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.054016590118408\n","Val loss: 1079.13427734375\n","Epoch: 225, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 5.873513221740723\n","Val loss: 777.51904296875\n","Epoch: 226, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.058165550231934\n","Val loss: 770.1473388671875\n","Epoch: 227, Train Acc: 0.9744, Validation Acc: 0.9674\n","Train loss: 6.0240912437438965\n","Val loss: 666.5133666992188\n","Epoch: 228, Train Acc: 0.9744, Validation Acc: 0.9674\n","Train loss: 9.10732364654541\n","Val loss: 768.2892456054688\n","Epoch: 229, Train Acc: 0.9744, Validation Acc: 0.9642\n","Train loss: 6.169833660125732\n","Val loss: 772.3960571289062\n","Epoch: 230, Train Acc: 0.9744, Validation Acc: 0.9642\n","Train loss: 6.066104412078857\n","Val loss: 800.1080932617188\n","Epoch: 231, Train Acc: 0.9744, Validation Acc: 0.9642\n","Train loss: 5.990494251251221\n","Val loss: 808.5275268554688\n","Epoch: 232, Train Acc: 0.9744, Validation Acc: 0.9674\n","Train loss: 5.941647052764893\n","Val loss: 976.73193359375\n","Epoch: 233, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 5.886434555053711\n","Val loss: 842.583251953125\n","Epoch: 234, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 5.870187282562256\n","Val loss: 858.0230712890625\n","Epoch: 235, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 5.858431339263916\n","Val loss: 865.7123413085938\n","Epoch: 236, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 5.902946472167969\n","Val loss: 933.6184692382812\n","Epoch: 237, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 6.632783889770508\n","Val loss: 843.7189331054688\n","Epoch: 238, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 7.500231742858887\n","Val loss: 732.2125854492188\n","Epoch: 239, Train Acc: 0.9744, Validation Acc: 0.9625\n","Train loss: 6.738201141357422\n","Val loss: 719.23779296875\n","Epoch: 240, Train Acc: 0.9744, Validation Acc: 0.9625\n","Train loss: 6.304542064666748\n","Val loss: 772.4540405273438\n","Epoch: 241, Train Acc: 0.9744, Validation Acc: 0.9642\n","Train loss: 6.0689287185668945\n","Val loss: 774.644287109375\n","Epoch: 242, Train Acc: 0.9744, Validation Acc: 0.9642\n","Train loss: 5.945788860321045\n","Val loss: 793.959228515625\n","Epoch: 243, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 8.762941360473633\n","Val loss: 791.6158447265625\n","Epoch: 244, Train Acc: 0.9744, Validation Acc: 0.9642\n","Train loss: 5.959843635559082\n","Val loss: 807.3782348632812\n","Epoch: 245, Train Acc: 0.9744, Validation Acc: 0.9642\n","Train loss: 6.227703094482422\n","Val loss: 720.6226806640625\n","Epoch: 246, Train Acc: 0.9744, Validation Acc: 0.9658\n","Train loss: 6.2070512771606445\n","Val loss: 706.1572875976562\n","Epoch: 247, Train Acc: 0.9744, Validation Acc: 0.9674\n","Train loss: 6.308096408843994\n","Val loss: 762.147216796875\n","Epoch: 248, Train Acc: 0.9744, Validation Acc: 0.9674\n","Train loss: 6.466385841369629\n","Val loss: 659.5218505859375\n","Epoch: 249, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 6.140857219696045\n","Val loss: 679.3070678710938\n","Epoch: 250, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 6.213847637176514\n","Val loss: 882.6444091796875\n","Epoch: 251, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 5.964057445526123\n","Val loss: 929.9878540039062\n","Epoch: 252, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 5.978143215179443\n","Val loss: 858.9625244140625\n","Epoch: 253, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 6.142916202545166\n","Val loss: 852.85302734375\n","Epoch: 254, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 6.128542900085449\n","Val loss: 856.5283203125\n","Epoch: 255, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 5.9880051612854\n","Val loss: 860.25341796875\n","Epoch: 256, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 5.909512042999268\n","Val loss: 894.3882446289062\n","Epoch: 257, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 5.619858741760254\n","Val loss: 895.4357299804688\n","Epoch: 258, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 5.774280548095703\n","Val loss: 923.1790771484375\n","Epoch: 259, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 6.216653823852539\n","Val loss: 1062.3919677734375\n","Epoch: 260, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 6.5546746253967285\n","Val loss: 886.4360961914062\n","Epoch: 261, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 6.468245029449463\n","Val loss: 926.8310546875\n","Epoch: 262, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 6.419873237609863\n","Val loss: 909.556884765625\n","Epoch: 263, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 6.3849968910217285\n","Val loss: 920.4813842773438\n","Epoch: 264, Train Acc: 0.9746, Validation Acc: 0.9691\n","Train loss: 6.27174711227417\n","Val loss: 946.794189453125\n","Epoch: 265, Train Acc: 0.9746, Validation Acc: 0.9691\n","Train loss: 6.0213541984558105\n","Val loss: 1096.3455810546875\n","Epoch: 266, Train Acc: 0.9746, Validation Acc: 0.9691\n","Train loss: 6.078525543212891\n","Val loss: 960.619873046875\n","Epoch: 267, Train Acc: 0.9746, Validation Acc: 0.9691\n","Train loss: 6.013348579406738\n","Val loss: 840.7846069335938\n","Epoch: 268, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 5.847290992736816\n","Val loss: 868.6885986328125\n","Epoch: 269, Train Acc: 0.9744, Validation Acc: 0.9691\n","Train loss: 6.0106520652771\n","Val loss: 893.3126831054688\n","Epoch: 270, Train Acc: 0.9746, Validation Acc: 0.9691\n","Train loss: 6.28481388092041\n","Val loss: 903.0221557617188\n","Epoch: 271, Train Acc: 0.9746, Validation Acc: 0.9691\n","Train loss: 6.297277927398682\n","Val loss: 931.0963134765625\n","Epoch: 272, Train Acc: 0.9746, Validation Acc: 0.9691\n","Train loss: 6.262930870056152\n","Val loss: 944.0966796875\n","Epoch: 273, Train Acc: 0.9746, Validation Acc: 0.9691\n","Train loss: 5.982912540435791\n","Val loss: 969.9423828125\n","Epoch: 274, Train Acc: 0.9746, Validation Acc: 0.9691\n","Train loss: 5.739162445068359\n","Val loss: 1216.62353515625\n","Epoch: 275, Train Acc: 0.9746, Validation Acc: 0.9691\n","Train loss: 7.284821033477783\n","Val loss: 560.4043579101562\n","Epoch: 276, Train Acc: 0.9746, Validation Acc: 0.9691\n","Train loss: 6.204380989074707\n","Val loss: 708.87646484375\n","Epoch: 277, Train Acc: 0.9746, Validation Acc: 0.9691\n","Train loss: 6.090760707855225\n","Val loss: 883.3509521484375\n","Epoch: 278, Train Acc: 0.9749, Validation Acc: 0.9691\n","Train loss: 5.828963756561279\n","Val loss: 846.5958251953125\n","Epoch: 279, Train Acc: 0.9749, Validation Acc: 0.9691\n","Train loss: 6.097516059875488\n","Val loss: 1365.0341796875\n","Epoch: 280, Train Acc: 0.9749, Validation Acc: 0.9691\n","Train loss: 5.882160186767578\n","Val loss: 1396.685546875\n","Epoch: 281, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.70697021484375\n","Val loss: 1341.5799560546875\n","Epoch: 282, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.904757022857666\n","Val loss: 1331.534912109375\n","Epoch: 283, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 6.123177528381348\n","Val loss: 1306.87646484375\n","Epoch: 284, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.838566780090332\n","Val loss: 1563.405029296875\n","Epoch: 285, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.839498043060303\n","Val loss: 1303.0098876953125\n","Epoch: 286, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 6.02435827255249\n","Val loss: 1309.0440673828125\n","Epoch: 287, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.906444072723389\n","Val loss: 1352.21435546875\n","Epoch: 288, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.577417850494385\n","Val loss: 1378.05078125\n","Epoch: 289, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.857228755950928\n","Val loss: 1518.154296875\n","Epoch: 290, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.545732021331787\n","Val loss: 1470.566162109375\n","Epoch: 291, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.986874103546143\n","Val loss: 1295.7576904296875\n","Epoch: 292, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.563121318817139\n","Val loss: 1302.635986328125\n","Epoch: 293, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.833731174468994\n","Val loss: 1130.135009765625\n","Epoch: 294, Train Acc: 0.9749, Validation Acc: 0.9691\n","Train loss: 8.216158866882324\n","Val loss: 1177.1419677734375\n","Epoch: 295, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 6.0355706214904785\n","Val loss: 1280.2706298828125\n","Epoch: 296, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 6.007151126861572\n","Val loss: 1262.483642578125\n","Epoch: 297, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.9680562019348145\n","Val loss: 1337.3568115234375\n","Epoch: 298, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.9513983726501465\n","Val loss: 1235.870361328125\n","Epoch: 299, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.966850757598877\n","Val loss: 1260.4749755859375\n","Epoch: 300, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 6.048049449920654\n","Val loss: 1233.6048583984375\n","Epoch: 301, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.9782915115356445\n","Val loss: 1489.9605712890625\n","Epoch: 302, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 6.039401054382324\n","Val loss: 1187.728759765625\n","Epoch: 303, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.943351745605469\n","Val loss: 1132.0811767578125\n","Epoch: 304, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.9387712478637695\n","Val loss: 1163.1097412109375\n","Epoch: 305, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.936174392700195\n","Val loss: 1292.496826171875\n","Epoch: 306, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.931162357330322\n","Val loss: 1181.1846923828125\n","Epoch: 307, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.883350849151611\n","Val loss: 1487.2344970703125\n","Epoch: 308, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.903717517852783\n","Val loss: 1189.604736328125\n","Epoch: 309, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.871030330657959\n","Val loss: 1198.0478515625\n","Epoch: 310, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.890085220336914\n","Val loss: 1191.4931640625\n","Epoch: 311, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.852896213531494\n","Val loss: 1451.5562744140625\n","Epoch: 312, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.909482002258301\n","Val loss: 1207.8685302734375\n","Epoch: 313, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 6.066002368927002\n","Val loss: 1289.5289306640625\n","Epoch: 314, Train Acc: 0.9749, Validation Acc: 0.9723\n","Train loss: 5.854187488555908\n","Val loss: 1390.9095458984375\n","Epoch: 315, Train Acc: 0.9749, Validation Acc: 0.9723\n","Train loss: 5.904773712158203\n","Val loss: 1519.576171875\n","Epoch: 316, Train Acc: 0.9749, Validation Acc: 0.9723\n","Train loss: 5.81867790222168\n","Val loss: 1281.6746826171875\n","Epoch: 317, Train Acc: 0.9749, Validation Acc: 0.9723\n","Train loss: 5.807529926300049\n","Val loss: 1320.1417236328125\n","Epoch: 318, Train Acc: 0.9749, Validation Acc: 0.9723\n","Train loss: 5.818202972412109\n","Val loss: 1133.060791015625\n","Epoch: 319, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.7391276359558105\n","Val loss: 1091.784423828125\n","Epoch: 320, Train Acc: 0.9749, Validation Acc: 0.9723\n","Train loss: 5.64459228515625\n","Val loss: 1146.4302978515625\n","Epoch: 321, Train Acc: 0.9749, Validation Acc: 0.9723\n","Train loss: 5.50453519821167\n","Val loss: 1134.0921630859375\n","Epoch: 322, Train Acc: 0.9749, Validation Acc: 0.9723\n","Train loss: 5.747126579284668\n","Val loss: 1129.4818115234375\n","Epoch: 323, Train Acc: 0.9749, Validation Acc: 0.9723\n","Train loss: 5.689914703369141\n","Val loss: 1191.1640625\n","Epoch: 324, Train Acc: 0.9749, Validation Acc: 0.9723\n","Train loss: 7.570035934448242\n","Val loss: 830.407958984375\n","Epoch: 325, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.776939392089844\n","Val loss: 1213.565673828125\n","Epoch: 326, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.764033317565918\n","Val loss: 1305.5030517578125\n","Epoch: 327, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.759721279144287\n","Val loss: 1433.292236328125\n","Epoch: 328, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.707263946533203\n","Val loss: 1344.7637939453125\n","Epoch: 329, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.730834484100342\n","Val loss: 1459.19677734375\n","Epoch: 330, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.7165608406066895\n","Val loss: 1241.8990478515625\n","Epoch: 331, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 6.671480178833008\n","Val loss: 478.6746826171875\n","Epoch: 332, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.706679344177246\n","Val loss: 617.7966918945312\n","Epoch: 333, Train Acc: 0.9746, Validation Acc: 0.9691\n","Train loss: 5.70188570022583\n","Val loss: 619.19873046875\n","Epoch: 334, Train Acc: 0.9749, Validation Acc: 0.9691\n","Train loss: 5.616623878479004\n","Val loss: 608.8251953125\n","Epoch: 335, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.724441051483154\n","Val loss: 624.0029907226562\n","Epoch: 336, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.762425899505615\n","Val loss: 633.8360595703125\n","Epoch: 337, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 6.511679649353027\n","Val loss: 433.1344299316406\n","Epoch: 338, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.694105625152588\n","Val loss: 401.66278076171875\n","Epoch: 339, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.577602386474609\n","Val loss: 437.9436950683594\n","Epoch: 340, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.433745384216309\n","Val loss: 449.3025207519531\n","Epoch: 341, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 6.211427211761475\n","Val loss: 248.66098022460938\n","Epoch: 342, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 7.359161376953125\n","Val loss: 303.0700378417969\n","Epoch: 343, Train Acc: 0.9746, Validation Acc: 0.9691\n","Train loss: 6.456727504730225\n","Val loss: 237.49514770507812\n","Epoch: 344, Train Acc: 0.9746, Validation Acc: 0.9691\n","Train loss: 5.74099588394165\n","Val loss: 245.01608276367188\n","Epoch: 345, Train Acc: 0.9746, Validation Acc: 0.9691\n","Train loss: 5.7338175773620605\n","Val loss: 244.27688598632812\n","Epoch: 346, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.653929710388184\n","Val loss: 243.42669677734375\n","Epoch: 347, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.669988632202148\n","Val loss: 250.97203063964844\n","Epoch: 348, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.666616439819336\n","Val loss: 258.304931640625\n","Epoch: 349, Train Acc: 0.9759, Validation Acc: 0.9707\n","Train loss: 5.692060947418213\n","Val loss: 266.65594482421875\n","Epoch: 350, Train Acc: 0.9759, Validation Acc: 0.9707\n","Train loss: 6.841648101806641\n","Val loss: 293.97314453125\n","Epoch: 351, Train Acc: 0.9759, Validation Acc: 0.9707\n","Train loss: 5.641101837158203\n","Val loss: 276.0626525878906\n","Epoch: 352, Train Acc: 0.9759, Validation Acc: 0.9707\n","Train loss: 5.681558609008789\n","Val loss: 243.9690704345703\n","Epoch: 353, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.686514854431152\n","Val loss: 326.37896728515625\n","Epoch: 354, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.721410274505615\n","Val loss: 280.56256103515625\n","Epoch: 355, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.67996072769165\n","Val loss: 270.9279479980469\n","Epoch: 356, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.63045072555542\n","Val loss: 270.92938232421875\n","Epoch: 357, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.709061622619629\n","Val loss: 277.6806335449219\n","Epoch: 358, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.610651016235352\n","Val loss: 273.3747863769531\n","Epoch: 359, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.706809043884277\n","Val loss: 277.3562927246094\n","Epoch: 360, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.651412487030029\n","Val loss: 290.1956481933594\n","Epoch: 361, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.604416370391846\n","Val loss: 303.88677978515625\n","Epoch: 362, Train Acc: 0.9749, Validation Acc: 0.9707\n","Train loss: 5.659921169281006\n","Val loss: 288.90069580078125\n","Epoch: 363, Train Acc: 0.9759, Validation Acc: 0.9707\n","Train loss: 5.6326751708984375\n","Val loss: 291.139404296875\n","Epoch: 364, Train Acc: 0.9759, Validation Acc: 0.9707\n","Train loss: 5.591084957122803\n","Val loss: 302.4778137207031\n","Epoch: 365, Train Acc: 0.9759, Validation Acc: 0.9707\n","Train loss: 5.66522216796875\n","Val loss: 310.8706970214844\n","Epoch: 366, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.604227542877197\n","Val loss: 322.71630859375\n","Epoch: 367, Train Acc: 0.9759, Validation Acc: 0.9707\n","Train loss: 5.595333099365234\n","Val loss: 319.5544738769531\n","Epoch: 368, Train Acc: 0.9759, Validation Acc: 0.9707\n","Train loss: 5.56463098526001\n","Val loss: 336.1983947753906\n","Epoch: 369, Train Acc: 0.9759, Validation Acc: 0.9707\n","Train loss: 5.611722946166992\n","Val loss: 383.6478576660156\n","Epoch: 370, Train Acc: 0.9759, Validation Acc: 0.9707\n","Train loss: 5.5786871910095215\n","Val loss: 502.6748962402344\n","Epoch: 371, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.5712151527404785\n","Val loss: 439.9409484863281\n","Epoch: 372, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.5323615074157715\n","Val loss: 445.7247314453125\n","Epoch: 373, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.508461952209473\n","Val loss: 445.2217102050781\n","Epoch: 374, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.510690212249756\n","Val loss: 442.7580871582031\n","Epoch: 375, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.538839817047119\n","Val loss: 425.600341796875\n","Epoch: 376, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.550665378570557\n","Val loss: 458.8330383300781\n","Epoch: 377, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.584074974060059\n","Val loss: 401.09832763671875\n","Epoch: 378, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.530534744262695\n","Val loss: 403.4954833984375\n","Epoch: 379, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.546180725097656\n","Val loss: 403.1104736328125\n","Epoch: 380, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.526200771331787\n","Val loss: 475.5990905761719\n","Epoch: 381, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.470415115356445\n","Val loss: 447.5472412109375\n","Epoch: 382, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.595020294189453\n","Val loss: 402.27288818359375\n","Epoch: 383, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.47826623916626\n","Val loss: 403.4431457519531\n","Epoch: 384, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.530332565307617\n","Val loss: 405.1939697265625\n","Epoch: 385, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.475025177001953\n","Val loss: 411.54522705078125\n","Epoch: 386, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.474307537078857\n","Val loss: 445.7140197753906\n","Epoch: 387, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.495638370513916\n","Val loss: 448.0755310058594\n","Epoch: 388, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.571838855743408\n","Val loss: 414.2007141113281\n","Epoch: 389, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.502551555633545\n","Val loss: 503.96075439453125\n","Epoch: 390, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.523373126983643\n","Val loss: 520.46435546875\n","Epoch: 391, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.512153148651123\n","Val loss: 481.49981689453125\n","Epoch: 392, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.479115009307861\n","Val loss: 430.67230224609375\n","Epoch: 393, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.5645527839660645\n","Val loss: 401.5244445800781\n","Epoch: 394, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.506665229797363\n","Val loss: 377.0632629394531\n","Epoch: 395, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.5253095626831055\n","Val loss: 376.9736633300781\n","Epoch: 396, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.497145652770996\n","Val loss: 372.7537841796875\n","Epoch: 397, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.537386894226074\n","Val loss: 456.743896484375\n","Epoch: 398, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.476159572601318\n","Val loss: 400.37677001953125\n","Epoch: 399, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.462010860443115\n","Val loss: 398.8254699707031\n","Epoch: 400, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.543686866760254\n","Val loss: 407.8335266113281\n","Epoch: 401, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.509641170501709\n","Val loss: 486.2204284667969\n","Epoch: 402, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.472754001617432\n","Val loss: 512.6672973632812\n","Epoch: 403, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.441167831420898\n","Val loss: 536.991455078125\n","Epoch: 404, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.46715784072876\n","Val loss: 509.2747497558594\n","Epoch: 405, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 31.981645584106445\n","Val loss: 682.6852416992188\n","Epoch: 406, Train Acc: 0.9759, Validation Acc: 0.9707\n","Train loss: 6.432806015014648\n","Val loss: 486.79840087890625\n","Epoch: 407, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.4527106285095215\n","Val loss: 577.1627807617188\n","Epoch: 408, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.477367877960205\n","Val loss: 470.0303955078125\n","Epoch: 409, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.428871154785156\n","Val loss: 471.3219299316406\n","Epoch: 410, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.463010787963867\n","Val loss: 476.2647399902344\n","Epoch: 411, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.416673183441162\n","Val loss: 478.1008605957031\n","Epoch: 412, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.444391250610352\n","Val loss: 582.6300048828125\n","Epoch: 413, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.48887825012207\n","Val loss: 535.113037109375\n","Epoch: 414, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.395851135253906\n","Val loss: 580.8779907226562\n","Epoch: 415, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.464864730834961\n","Val loss: 507.4609069824219\n","Epoch: 416, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.4517621994018555\n","Val loss: 545.383056640625\n","Epoch: 417, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.391071319580078\n","Val loss: 512.456787109375\n","Epoch: 418, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.403730869293213\n","Val loss: 551.1007690429688\n","Epoch: 419, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.410890579223633\n","Val loss: 541.8299560546875\n","Epoch: 420, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.391526222229004\n","Val loss: 536.2959594726562\n","Epoch: 421, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.401132583618164\n","Val loss: 629.4979858398438\n","Epoch: 422, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.627889156341553\n","Val loss: 413.82415771484375\n","Epoch: 423, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.709041595458984\n","Val loss: 469.6402893066406\n","Epoch: 424, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.42781400680542\n","Val loss: 459.20513916015625\n","Epoch: 425, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.370951175689697\n","Val loss: 454.49029541015625\n","Epoch: 426, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 8.605616569519043\n","Val loss: 468.9539489746094\n","Epoch: 427, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.369499683380127\n","Val loss: 546.22314453125\n","Epoch: 428, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.373633861541748\n","Val loss: 483.8000793457031\n","Epoch: 429, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.348327159881592\n","Val loss: 498.3115539550781\n","Epoch: 430, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.403226852416992\n","Val loss: 904.9804077148438\n","Epoch: 431, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.375524997711182\n","Val loss: 1073.9176025390625\n","Epoch: 432, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.349157333374023\n","Val loss: 977.1205444335938\n","Epoch: 433, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.364290714263916\n","Val loss: 968.537353515625\n","Epoch: 434, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.5444159507751465\n","Val loss: 767.7485961914062\n","Epoch: 435, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.344534873962402\n","Val loss: 771.3606567382812\n","Epoch: 436, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.387370586395264\n","Val loss: 792.7359619140625\n","Epoch: 437, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.383146286010742\n","Val loss: 1079.0673828125\n","Epoch: 438, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.32961368560791\n","Val loss: 809.79638671875\n","Epoch: 439, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.326359748840332\n","Val loss: 835.8140869140625\n","Epoch: 440, Train Acc: 0.9759, Validation Acc: 0.9707\n","Train loss: 5.323942184448242\n","Val loss: 899.4646606445312\n","Epoch: 441, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.402902603149414\n","Val loss: 769.3209838867188\n","Epoch: 442, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.366850852966309\n","Val loss: 742.6422119140625\n","Epoch: 443, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.350793361663818\n","Val loss: 757.8549194335938\n","Epoch: 444, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.344407081604004\n","Val loss: 755.400634765625\n","Epoch: 445, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.325777053833008\n","Val loss: 753.36083984375\n","Epoch: 446, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.343245983123779\n","Val loss: 914.5045166015625\n","Epoch: 447, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.308211326599121\n","Val loss: 777.5643920898438\n","Epoch: 448, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.3755364418029785\n","Val loss: 773.8178100585938\n","Epoch: 449, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.341564178466797\n","Val loss: 716.571533203125\n","Epoch: 450, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.321069717407227\n","Val loss: 721.5435180664062\n","Epoch: 451, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.332040309906006\n","Val loss: 751.898193359375\n","Epoch: 452, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.4312849044799805\n","Val loss: 719.3209228515625\n","Epoch: 453, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.348320484161377\n","Val loss: 720.5249633789062\n","Epoch: 454, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.35380220413208\n","Val loss: 713.4412231445312\n","Epoch: 455, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.3565239906311035\n","Val loss: 829.8917236328125\n","Epoch: 456, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.354175567626953\n","Val loss: 722.4076538085938\n","Epoch: 457, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.324859619140625\n","Val loss: 744.526611328125\n","Epoch: 458, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.312098503112793\n","Val loss: 742.0504760742188\n","Epoch: 459, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.31278133392334\n","Val loss: 863.1665649414062\n","Epoch: 460, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.317283630371094\n","Val loss: 745.212890625\n","Epoch: 461, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.371047019958496\n","Val loss: 838.3703002929688\n","Epoch: 462, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.33012056350708\n","Val loss: 721.232666015625\n","Epoch: 463, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.29799747467041\n","Val loss: 830.308349609375\n","Epoch: 464, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.300539493560791\n","Val loss: 843.1781616210938\n","Epoch: 465, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.347892761230469\n","Val loss: 794.6563720703125\n","Epoch: 466, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.313864231109619\n","Val loss: 763.810791015625\n","Epoch: 467, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.810499668121338\n","Val loss: 758.100830078125\n","Epoch: 468, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.291258811950684\n","Val loss: 768.5699462890625\n","Epoch: 469, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.3371663093566895\n","Val loss: 773.5245971679688\n","Epoch: 470, Train Acc: 0.9759, Validation Acc: 0.9707\n","Train loss: 5.299899101257324\n","Val loss: 807.4102783203125\n","Epoch: 471, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.346197128295898\n","Val loss: 749.6381225585938\n","Epoch: 472, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.339061737060547\n","Val loss: 770.997314453125\n","Epoch: 473, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 6.938959121704102\n","Val loss: 1242.6678466796875\n","Epoch: 474, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 7.381439208984375\n","Val loss: 817.674072265625\n","Epoch: 475, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.569159507751465\n","Val loss: 2734.75537109375\n","Epoch: 476, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.330775737762451\n","Val loss: 3121.3203125\n","Epoch: 477, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.360335350036621\n","Val loss: 2177.5400390625\n","Epoch: 478, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.326564788818359\n","Val loss: 2332.2314453125\n","Epoch: 479, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.4394378662109375\n","Val loss: 1827.70263671875\n","Epoch: 480, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 6.4164323806762695\n","Val loss: 1795.443115234375\n","Epoch: 481, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.296302795410156\n","Val loss: 1733.143798828125\n","Epoch: 482, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 7.225842475891113\n","Val loss: 2098.68359375\n","Epoch: 483, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.302742004394531\n","Val loss: 1789.2523193359375\n","Epoch: 484, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.301496982574463\n","Val loss: 2142.4296875\n","Epoch: 485, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.292601585388184\n","Val loss: 2087.106201171875\n","Epoch: 486, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.280561447143555\n","Val loss: 1813.0718994140625\n","Epoch: 487, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.2621965408325195\n","Val loss: 1822.1685791015625\n","Epoch: 488, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.328785419464111\n","Val loss: 1762.231201171875\n","Epoch: 489, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.295088768005371\n","Val loss: 2198.709228515625\n","Epoch: 490, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.441130638122559\n","Val loss: 1886.856201171875\n","Epoch: 491, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.263147354125977\n","Val loss: 1911.07568359375\n","Epoch: 492, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.293435573577881\n","Val loss: 1920.856201171875\n","Epoch: 493, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.271209239959717\n","Val loss: 1934.9268798828125\n","Epoch: 494, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.329047203063965\n","Val loss: 2354.200927734375\n","Epoch: 495, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.349869251251221\n","Val loss: 1781.609130859375\n","Epoch: 496, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.297714710235596\n","Val loss: 2147.648193359375\n","Epoch: 497, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.280467510223389\n","Val loss: 1807.046630859375\n","Epoch: 498, Train Acc: 0.9759, Validation Acc: 0.9723\n","Train loss: 5.2750701904296875\n","Val loss: 1848.5404052734375\n","Epoch: 499, Train Acc: 0.9759, Validation Acc: 0.9723\n"]}]},{"cell_type":"code","source":["print(train_accs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rU1eMLKZ-ipK","executionInfo":{"status":"ok","timestamp":1652892426657,"user_tz":-420,"elapsed":90,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}},"outputId":"7bfc62c4-16fb-431a-b530-38b5843f2820"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.9709099950273495, 0.9738935852809547, 0.9741422178020885, 0.9738935852809547, 0.9738935852809547, 0.9738935852809547, 0.9738935852809547, 0.9679264047737444, 0.9699154649428146, 0.9699154649428146, 0.9674291397314769, 0.9674291397314769, 0.9614619592242666, 0.9689209348582795, 0.9664346096469418, 0.9641969169567379, 0.9530084535057185, 0.9549975136747887, 0.9614619592242666, 0.9358528095474888, 0.8483341621084037, 0.8530581800099453, 0.8679761312779711, 0.8446046742913973, 0.8729487817006465, 0.8781700646444555, 0.8779214321233217, 0.9067628045748384, 0.8423669816011934, 0.8336648433615117, 0.8756837394331178, 0.8679761312779711, 0.8980606663351567, 0.8774241670810542, 0.8896071606166086, 0.8856290402784684, 0.894579811039284, 0.8970661362506216, 0.9132272501243163, 0.9333664843361512, 0.9343610144206862, 0.9351069119840875, 0.9582297364495276, 0.9617105917454003, 0.951516658378916, 0.9525111884634511, 0.9706613625062158, 0.9701640974639483, 0.9684236698160119, 0.9679264047737444, 0.9691695673794132, 0.9584783689706614, 0.9602187966185977, 0.9599701640974639, 0.9696668324216807, 0.9264047737444058, 0.9686723023371457, 0.9689209348582795, 0.969418199900547, 0.9684236698160119, 0.9666832421680756, 0.9659373446046743, 0.9656887120835406, 0.9649428145201392, 0.9646941819990055, 0.9632023868722028, 0.965191447041273, 0.9704127299850821, 0.9704127299850821, 0.969418199900547, 0.9689209348582795, 0.9686723023371457, 0.9689209348582795, 0.9686723023371457, 0.969418199900547, 0.969418199900547, 0.969418199900547, 0.9691695673794132, 0.9686723023371457, 0.9689209348582795, 0.9691695673794132, 0.969418199900547, 0.9686723023371457, 0.9684236698160119, 0.9696668324216807, 0.9699154649428146, 0.969418199900547, 0.9701640974639483, 0.9706613625062158, 0.9711586275484834, 0.9716558925907509, 0.9709099950273495, 0.9706613625062158, 0.9709099950273495, 0.9706613625062158, 0.9711586275484834, 0.9709099950273495, 0.9709099950273495, 0.9709099950273495, 0.9709099950273495, 0.9711586275484834, 0.9711586275484834, 0.9711586275484834, 0.9711586275484834, 0.9714072600696171, 0.9716558925907509, 0.9714072600696171, 0.9714072600696171, 0.9721531576330183, 0.9721531576330183, 0.9719045251118846, 0.9721531576330183, 0.9719045251118846, 0.9719045251118846, 0.9719045251118846, 0.9721531576330183, 0.9721531576330183, 0.9721531576330183, 0.9719045251118846, 0.9721531576330183, 0.9721531576330183, 0.9719045251118846, 0.9721531576330183, 0.9719045251118846, 0.9719045251118846, 0.9721531576330183, 0.9721531576330183, 0.9721531576330183, 0.9721531576330183, 0.9724017901541522, 0.9721531576330183, 0.9676777722526106, 0.9706613625062158, 0.9719045251118846, 0.9719045251118846, 0.9719045251118846, 0.9719045251118846, 0.9721531576330183, 0.9721531576330183, 0.9724017901541522, 0.9721531576330183, 0.9721531576330183, 0.9721531576330183, 0.9721531576330183, 0.9724017901541522, 0.9724017901541522, 0.9721531576330183, 0.9724017901541522, 0.9721531576330183, 0.9721531576330183, 0.9721531576330183, 0.9721531576330183, 0.9721531576330183, 0.9724017901541522, 0.9721531576330183, 0.9721531576330183, 0.9721531576330183, 0.9721531576330183, 0.9721531576330183, 0.9721531576330183, 0.9721531576330183, 0.9721531576330183, 0.9721531576330183, 0.9726504226752859, 0.9726504226752859, 0.9726504226752859, 0.9726504226752859, 0.9726504226752859, 0.9731476877175534, 0.9733963202386873, 0.9733963202386873, 0.9733963202386873, 0.9733963202386873, 0.9743908503232223, 0.9743908503232223, 0.9733963202386873, 0.9733963202386873, 0.9733963202386873, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9743908503232223, 0.9746394828443561, 0.9746394828443561, 0.9746394828443561, 0.9746394828443561, 0.9743908503232223, 0.9743908503232223, 0.9746394828443561, 0.9746394828443561, 0.9746394828443561, 0.9746394828443561, 0.9746394828443561, 0.9746394828443561, 0.9746394828443561, 0.9746394828443561, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9746394828443561, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9746394828443561, 0.9746394828443561, 0.9746394828443561, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9748881153654898, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249, 0.9758826454500249]\n"]}]},{"cell_type":"code","source":["print(val_accs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_3XyENTS-lVx","executionInfo":{"status":"ok","timestamp":1652892426661,"user_tz":-420,"elapsed":71,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}},"outputId":"c5190903-6d38-4fdd-a78a-eb22b916c9d9"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9723127035830619, 0.9723127035830619, 0.9706840390879479, 0.9690553745928339, 0.9657980456026058, 0.9641693811074918, 0.9674267100977199, 0.9657980456026058, 0.9641693811074918, 0.9560260586319218, 0.9674267100977199, 0.9657980456026058, 0.9609120521172638, 0.9527687296416938, 0.9478827361563518, 0.9560260586319218, 0.9185667752442996, 0.8224755700325733, 0.8403908794788274, 0.8599348534201955, 0.8273615635179153, 0.8517915309446255, 0.8615635179153095, 0.8648208469055375, 0.8990228013029316, 0.8208469055374593, 0.8159609120521173, 0.8664495114006515, 0.8680781758957655, 0.9071661237785016, 0.8811074918566775, 0.9039087947882736, 0.9022801302931596, 0.9071661237785016, 0.9055374592833876, 0.9299674267100977, 0.9429967426710097, 0.9446254071661238, 0.9429967426710097, 0.9657980456026058, 0.9625407166123778, 0.9625407166123778, 0.9592833876221498, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9609120521172638, 0.9592833876221498, 0.9657980456026058, 0.9153094462540716, 0.9625407166123778, 0.9625407166123778, 0.9625407166123778, 0.9657980456026058, 0.9641693811074918, 0.9641693811074918, 0.9609120521172638, 0.9625407166123778, 0.9592833876221498, 0.9592833876221498, 0.9625407166123778, 0.9625407166123778, 0.9625407166123778, 0.9641693811074918, 0.9641693811074918, 0.9625407166123778, 0.9609120521172638, 0.9609120521172638, 0.9641693811074918, 0.9641693811074918, 0.9625407166123778, 0.9641693811074918, 0.9625407166123778, 0.9625407166123778, 0.9625407166123778, 0.9641693811074918, 0.9625407166123778, 0.9625407166123778, 0.9641693811074918, 0.9641693811074918, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9641693811074918, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9641693811074918, 0.9641693811074918, 0.9625407166123778, 0.9625407166123778, 0.9625407166123778, 0.9625407166123778, 0.9609120521172638, 0.9609120521172638, 0.9609120521172638, 0.9609120521172638, 0.9609120521172638, 0.9609120521172638, 0.9625407166123778, 0.9625407166123778, 0.9657980456026058, 0.9641693811074918, 0.9609120521172638, 0.9609120521172638, 0.9609120521172638, 0.9609120521172638, 0.9609120521172638, 0.9609120521172638, 0.9625407166123778, 0.9609120521172638, 0.9609120521172638, 0.9609120521172638, 0.9609120521172638, 0.9609120521172638, 0.9609120521172638, 0.9609120521172638, 0.9609120521172638, 0.9609120521172638, 0.9609120521172638, 0.9609120521172638, 0.9511400651465798, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9674267100977199, 0.9674267100977199, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9674267100977199, 0.9674267100977199, 0.9674267100977199, 0.9657980456026058, 0.9674267100977199, 0.9674267100977199, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9674267100977199, 0.9674267100977199, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9674267100977199, 0.9674267100977199, 0.9674267100977199, 0.9690553745928339, 0.9674267100977199, 0.9657980456026058, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9657980456026058, 0.9657980456026058, 0.9657980456026058, 0.9674267100977199, 0.9674267100977199, 0.9641693811074918, 0.9641693811074918, 0.9641693811074918, 0.9674267100977199, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9625407166123778, 0.9625407166123778, 0.9641693811074918, 0.9641693811074918, 0.9690553745928339, 0.9641693811074918, 0.9641693811074918, 0.9657980456026058, 0.9674267100977199, 0.9674267100977199, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9690553745928339, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9706840390879479, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9690553745928339, 0.9690553745928339, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9690553745928339, 0.9690553745928339, 0.9690553745928339, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9723127035830619, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9706840390879479, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9706840390879479, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9706840390879479, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9706840390879479, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619, 0.9723127035830619]\n"]}]},{"cell_type":"code","source":["print(val_losses)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vOpZ3N_l-oio","executionInfo":{"status":"ok","timestamp":1652892426662,"user_tz":-420,"elapsed":51,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}},"outputId":"bb00f188-5b59-41ec-f379-3b73bc1ff7d0"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[660.9125366210938, 516.7020263671875, 454.8880310058594, 403.0007629394531, 370.2259216308594, 347.3230285644531, 305.7716064453125, 324.3023376464844, 305.1994934082031, 310.1129455566406, 319.08160400390625, 292.1357421875, 345.5818176269531, 362.4306640625, 295.3589172363281, 301.8414001464844, 304.9920349121094, 299.0663757324219, 296.0998229980469, 305.1444091796875, 316.06170654296875, 302.4713134765625, 285.63641357421875, 286.564697265625, 286.8572082519531, 350.71728515625, 287.5487976074219, 287.2823791503906, 298.6003112792969, 291.8998107910156, 281.1280822753906, 328.7937927246094, 294.3521728515625, 296.1820983886719, 281.4567565917969, 331.7198791503906, 280.7315673828125, 283.5914001464844, 281.52850341796875, 332.52008056640625, 306.86737060546875, 304.8620300292969, 298.528564453125, 303.17315673828125, 348.14141845703125, 348.5089416503906, 301.0387268066406, 342.4584655761719, 375.135986328125, 315.1614074707031, 297.37261962890625, 295.02655029296875, 299.16485595703125, 300.691650390625, 295.72650146484375, 298.18646240234375, 354.58587646484375, 305.2722473144531, 410.88787841796875, 310.5780334472656, 301.30047607421875, 283.9664306640625, 283.9378662109375, 286.44580078125, 289.46307373046875, 355.544189453125, 290.427734375, 238.41383361816406, 244.2801971435547, 249.04263305664062, 249.8215789794922, 248.642578125, 291.2464904785156, 251.06004333496094, 254.09339904785156, 257.9763488769531, 261.8548889160156, 262.9699401855469, 262.9338073730469, 266.2635803222656, 267.7279357910156, 272.7896423339844, 275.4664306640625, 317.7938232421875, 352.2359619140625, 255.7036895751953, 258.8467102050781, 263.9148864746094, 278.2952575683594, 271.1930847167969, 279.5157775878906, 266.45062255859375, 291.0405578613281, 320.10577392578125, 302.7632751464844, 267.0023193359375, 274.0102233886719, 278.655517578125, 322.03826904296875, 300.4105529785156, 287.1369323730469, 350.42437744140625, 302.42352294921875, 300.03582763671875, 319.5537109375, 304.5946960449219, 396.53204345703125, 357.0990905761719, 308.59564208984375, 314.8035888671875, 321.41802978515625, 327.06207275390625, 333.936279296875, 353.2453308105469, 385.1011047363281, 353.02960205078125, 370.9439392089844, 353.6406555175781, 374.7071838378906, 359.2950134277344, 368.6587829589844, 367.9853515625, 367.6466369628906, 347.150390625, 350.2715148925781, 349.3343200683594, 359.7692565917969, 361.4680480957031, 360.56390380859375, 364.62274169921875, 389.4179382324219, 218.8712615966797, 691.8228759765625, 628.783935546875, 631.2822265625, 669.2667236328125, 637.1881713867188, 634.37353515625, 699.1676025390625, 667.6068725585938, 663.8203735351562, 783.1177978515625, 673.732421875, 675.2627563476562, 668.6861572265625, 662.8234252929688, 735.138671875, 676.9534301757812, 701.6472778320312, 659.1781005859375, 667.6942749023438, 657.01416015625, 663.6520385742188, 670.2793579101562, 867.8076171875, 705.2733154296875, 683.0154418945312, 685.6539916992188, 690.2734375, 691.1959838867188, 700.3091430664062, 738.1588134765625, 699.1064453125, 705.8331298828125, 700.7345581054688, 823.8521728515625, 697.8711547851562, 696.9663696289062, 686.316650390625, 664.9749145507812, 820.3756103515625, 670.9539794921875, 857.12158203125, 694.5158081054688, 700.8077392578125, 725.69970703125, 672.0775756835938, 676.7022705078125, 687.49560546875, 749.5316162109375, 713.4708251953125, 798.5076904296875, 849.076416015625, 764.0910034179688, 784.6154174804688, 814.7597045898438, 864.2825317382812, 909.6494140625, 701.4832153320312, 879.7796020507812, 731.13330078125, 751.3772583007812, 788.242431640625, 876.2894287109375, 805.7919311523438, 749.63525390625, 845.6832275390625, 990.1207275390625, 958.7978515625, 905.8574829101562, 971.480224609375, 870.54833984375, 902.342529296875, 973.7049560546875, 909.6906127929688, 886.5707397460938, 897.0757446289062, 903.4295043945312, 998.979248046875, 843.5887451171875, 1070.42236328125, 849.3079223632812, 917.02001953125, 882.6168212890625, 968.937255859375, 949.4232788085938, 1091.7132568359375, 1011.0349731445312, 597.6415405273438, 672.0939331054688, 846.824951171875, 701.3418579101562, 761.9962158203125, 782.3265380859375, 837.8944702148438, 1079.13427734375, 777.51904296875, 770.1473388671875, 666.5133666992188, 768.2892456054688, 772.3960571289062, 800.1080932617188, 808.5275268554688, 976.73193359375, 842.583251953125, 858.0230712890625, 865.7123413085938, 933.6184692382812, 843.7189331054688, 732.2125854492188, 719.23779296875, 772.4540405273438, 774.644287109375, 793.959228515625, 791.6158447265625, 807.3782348632812, 720.6226806640625, 706.1572875976562, 762.147216796875, 659.5218505859375, 679.3070678710938, 882.6444091796875, 929.9878540039062, 858.9625244140625, 852.85302734375, 856.5283203125, 860.25341796875, 894.3882446289062, 895.4357299804688, 923.1790771484375, 1062.3919677734375, 886.4360961914062, 926.8310546875, 909.556884765625, 920.4813842773438, 946.794189453125, 1096.3455810546875, 960.619873046875, 840.7846069335938, 868.6885986328125, 893.3126831054688, 903.0221557617188, 931.0963134765625, 944.0966796875, 969.9423828125, 1216.62353515625, 560.4043579101562, 708.87646484375, 883.3509521484375, 846.5958251953125, 1365.0341796875, 1396.685546875, 1341.5799560546875, 1331.534912109375, 1306.87646484375, 1563.405029296875, 1303.0098876953125, 1309.0440673828125, 1352.21435546875, 1378.05078125, 1518.154296875, 1470.566162109375, 1295.7576904296875, 1302.635986328125, 1130.135009765625, 1177.1419677734375, 1280.2706298828125, 1262.483642578125, 1337.3568115234375, 1235.870361328125, 1260.4749755859375, 1233.6048583984375, 1489.9605712890625, 1187.728759765625, 1132.0811767578125, 1163.1097412109375, 1292.496826171875, 1181.1846923828125, 1487.2344970703125, 1189.604736328125, 1198.0478515625, 1191.4931640625, 1451.5562744140625, 1207.8685302734375, 1289.5289306640625, 1390.9095458984375, 1519.576171875, 1281.6746826171875, 1320.1417236328125, 1133.060791015625, 1091.784423828125, 1146.4302978515625, 1134.0921630859375, 1129.4818115234375, 1191.1640625, 830.407958984375, 1213.565673828125, 1305.5030517578125, 1433.292236328125, 1344.7637939453125, 1459.19677734375, 1241.8990478515625, 478.6746826171875, 617.7966918945312, 619.19873046875, 608.8251953125, 624.0029907226562, 633.8360595703125, 433.1344299316406, 401.66278076171875, 437.9436950683594, 449.3025207519531, 248.66098022460938, 303.0700378417969, 237.49514770507812, 245.01608276367188, 244.27688598632812, 243.42669677734375, 250.97203063964844, 258.304931640625, 266.65594482421875, 293.97314453125, 276.0626525878906, 243.9690704345703, 326.37896728515625, 280.56256103515625, 270.9279479980469, 270.92938232421875, 277.6806335449219, 273.3747863769531, 277.3562927246094, 290.1956481933594, 303.88677978515625, 288.90069580078125, 291.139404296875, 302.4778137207031, 310.8706970214844, 322.71630859375, 319.5544738769531, 336.1983947753906, 383.6478576660156, 502.6748962402344, 439.9409484863281, 445.7247314453125, 445.2217102050781, 442.7580871582031, 425.600341796875, 458.8330383300781, 401.09832763671875, 403.4954833984375, 403.1104736328125, 475.5990905761719, 447.5472412109375, 402.27288818359375, 403.4431457519531, 405.1939697265625, 411.54522705078125, 445.7140197753906, 448.0755310058594, 414.2007141113281, 503.96075439453125, 520.46435546875, 481.49981689453125, 430.67230224609375, 401.5244445800781, 377.0632629394531, 376.9736633300781, 372.7537841796875, 456.743896484375, 400.37677001953125, 398.8254699707031, 407.8335266113281, 486.2204284667969, 512.6672973632812, 536.991455078125, 509.2747497558594, 682.6852416992188, 486.79840087890625, 577.1627807617188, 470.0303955078125, 471.3219299316406, 476.2647399902344, 478.1008605957031, 582.6300048828125, 535.113037109375, 580.8779907226562, 507.4609069824219, 545.383056640625, 512.456787109375, 551.1007690429688, 541.8299560546875, 536.2959594726562, 629.4979858398438, 413.82415771484375, 469.6402893066406, 459.20513916015625, 454.49029541015625, 468.9539489746094, 546.22314453125, 483.8000793457031, 498.3115539550781, 904.9804077148438, 1073.9176025390625, 977.1205444335938, 968.537353515625, 767.7485961914062, 771.3606567382812, 792.7359619140625, 1079.0673828125, 809.79638671875, 835.8140869140625, 899.4646606445312, 769.3209838867188, 742.6422119140625, 757.8549194335938, 755.400634765625, 753.36083984375, 914.5045166015625, 777.5643920898438, 773.8178100585938, 716.571533203125, 721.5435180664062, 751.898193359375, 719.3209228515625, 720.5249633789062, 713.4412231445312, 829.8917236328125, 722.4076538085938, 744.526611328125, 742.0504760742188, 863.1665649414062, 745.212890625, 838.3703002929688, 721.232666015625, 830.308349609375, 843.1781616210938, 794.6563720703125, 763.810791015625, 758.100830078125, 768.5699462890625, 773.5245971679688, 807.4102783203125, 749.6381225585938, 770.997314453125, 1242.6678466796875, 817.674072265625, 2734.75537109375, 3121.3203125, 2177.5400390625, 2332.2314453125, 1827.70263671875, 1795.443115234375, 1733.143798828125, 2098.68359375, 1789.2523193359375, 2142.4296875, 2087.106201171875, 1813.0718994140625, 1822.1685791015625, 1762.231201171875, 2198.709228515625, 1886.856201171875, 1911.07568359375, 1920.856201171875, 1934.9268798828125, 2354.200927734375, 1781.609130859375, 2147.648193359375, 1807.046630859375, 1848.5404052734375]\n"]}]},{"cell_type":"code","source":["print(train_losses)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5y4An-u3-pcM","executionInfo":{"status":"ok","timestamp":1652892426666,"user_tz":-420,"elapsed":36,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}},"outputId":"26d24527-b700-486c-8e14-b6dacdac4934"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[26745.9609375, 4260.84912109375, 2661.66259765625, 1900.03759765625, 1293.9578857421875, 980.3457641601562, 624.36279296875, 477.5736999511719, 320.51788330078125, 217.7150115966797, 181.69700622558594, 165.90663146972656, 79.41962432861328, 57.76284408569336, 56.6057014465332, 42.00975799560547, 23.255964279174805, 22.36056137084961, 26.709260940551758, 18.403564453125, 20.680160522460938, 13.788268089294434, 26.315088272094727, 16.037010192871094, 13.723410606384277, 13.403890609741211, 14.230494499206543, 12.616652488708496, 12.953502655029297, 11.969417572021484, 11.920855522155762, 11.876898765563965, 12.463939666748047, 11.539228439331055, 12.310253143310547, 10.899442672729492, 11.499069213867188, 11.812355995178223, 11.055170059204102, 13.0247220993042, 12.982739448547363, 12.623580932617188, 11.282968521118164, 10.53663158416748, 10.274467468261719, 10.133371353149414, 11.424302101135254, 11.066484451293945, 13.918051719665527, 9.755294799804688, 11.390628814697266, 9.814061164855957, 10.040131568908691, 10.374249458312988, 10.241103172302246, 10.021900177001953, 10.507002830505371, 9.855745315551758, 10.032814979553223, 12.328248023986816, 9.745230674743652, 9.57168197631836, 9.246232032775879, 9.165617942810059, 9.443720817565918, 8.725404739379883, 9.148048400878906, 11.335662841796875, 9.656485557556152, 10.491825103759766, 8.553786277770996, 9.11798095703125, 9.480308532714844, 9.020854949951172, 8.56604290008545, 8.781920433044434, 8.862713813781738, 8.506743431091309, 8.641844749450684, 8.2556791305542, 8.517890930175781, 8.513832092285156, 8.500876426696777, 8.703367233276367, 8.650773048400879, 10.06687068939209, 8.024120330810547, 8.814126014709473, 8.334653854370117, 8.962244033813477, 8.010629653930664, 8.394826889038086, 8.19405460357666, 7.90833854675293, 8.179108619689941, 8.033506393432617, 8.316191673278809, 7.8195600509643555, 8.140425682067871, 8.126569747924805, 8.067700386047363, 7.486990451812744, 7.761427402496338, 7.572740077972412, 7.717136859893799, 7.719760417938232, 7.9447760581970215, 7.74701452255249, 7.768251419067383, 7.4550323486328125, 7.340917587280273, 8.09188175201416, 8.507307052612305, 7.403536319732666, 7.537789344787598, 7.585190773010254, 7.843311309814453, 7.319466590881348, 8.154150009155273, 8.246682167053223, 7.495412826538086, 7.358823776245117, 7.453298568725586, 7.02710485458374, 7.197238445281982, 7.168181896209717, 7.36548376083374, 6.950474262237549, 7.182266712188721, 7.077373504638672, 7.20494270324707, 18.023597717285156, 15.816549301147461, 7.533098220825195, 6.979141712188721, 7.237344741821289, 6.85399866104126, 7.183920860290527, 7.0915985107421875, 6.953119277954102, 7.206900596618652, 6.9779052734375, 6.980799198150635, 6.65410041809082, 6.930095672607422, 6.589076042175293, 8.703629493713379, 7.239306926727295, 6.994627952575684, 6.979141712188721, 6.926133632659912, 7.389991760253906, 6.8510966300964355, 6.840684413909912, 6.534622669219971, 6.764252185821533, 6.720756530761719, 6.4407196044921875, 6.499542713165283, 6.5506696701049805, 6.612680435180664, 6.644730091094971, 6.655066013336182, 6.357009410858154, 6.342177867889404, 6.451056003570557, 6.605127334594727, 6.508824825286865, 6.463628768920898, 6.677018165588379, 6.791670322418213, 7.096391201019287, 6.841106414794922, 6.963905334472656, 6.795206069946289, 6.836000442504883, 6.6806464195251465, 6.845958709716797, 6.846567630767822, 6.708243370056152, 6.713286399841309, 6.458972930908203, 6.340682029724121, 6.132683277130127, 6.418070316314697, 6.2512359619140625, 6.350492477416992, 7.061882019042969, 6.405966758728027, 6.273232936859131, 6.090698719024658, 6.1495466232299805, 6.225568771362305, 6.269773960113525, 6.806335926055908, 6.307709693908691, 6.321626663208008, 6.017899036407471, 6.199193000793457, 7.000038146972656, 6.9932451248168945, 6.740139484405518, 6.232085704803467, 6.304470062255859, 6.033566474914551, 6.527647972106934, 6.071479797363281, 6.309773921966553, 6.056937217712402, 6.229587554931641, 6.003345012664795, 6.184157848358154, 6.276783466339111, 6.037020206451416, 6.192720413208008, 6.126056671142578, 6.395094871520996, 6.282619476318359, 10.78919792175293, 6.418344974517822, 6.269080638885498, 5.914449691772461, 6.364140510559082, 6.088985443115234, 6.109817981719971, 6.054016590118408, 5.873513221740723, 6.058165550231934, 6.0240912437438965, 9.10732364654541, 6.169833660125732, 6.066104412078857, 5.990494251251221, 5.941647052764893, 5.886434555053711, 5.870187282562256, 5.858431339263916, 5.902946472167969, 6.632783889770508, 7.500231742858887, 6.738201141357422, 6.304542064666748, 6.0689287185668945, 5.945788860321045, 8.762941360473633, 5.959843635559082, 6.227703094482422, 6.2070512771606445, 6.308096408843994, 6.466385841369629, 6.140857219696045, 6.213847637176514, 5.964057445526123, 5.978143215179443, 6.142916202545166, 6.128542900085449, 5.9880051612854, 5.909512042999268, 5.619858741760254, 5.774280548095703, 6.216653823852539, 6.5546746253967285, 6.468245029449463, 6.419873237609863, 6.3849968910217285, 6.27174711227417, 6.0213541984558105, 6.078525543212891, 6.013348579406738, 5.847290992736816, 6.0106520652771, 6.28481388092041, 6.297277927398682, 6.262930870056152, 5.982912540435791, 5.739162445068359, 7.284821033477783, 6.204380989074707, 6.090760707855225, 5.828963756561279, 6.097516059875488, 5.882160186767578, 5.70697021484375, 5.904757022857666, 6.123177528381348, 5.838566780090332, 5.839498043060303, 6.02435827255249, 5.906444072723389, 5.577417850494385, 5.857228755950928, 5.545732021331787, 5.986874103546143, 5.563121318817139, 5.833731174468994, 8.216158866882324, 6.0355706214904785, 6.007151126861572, 5.9680562019348145, 5.9513983726501465, 5.966850757598877, 6.048049449920654, 5.9782915115356445, 6.039401054382324, 5.943351745605469, 5.9387712478637695, 5.936174392700195, 5.931162357330322, 5.883350849151611, 5.903717517852783, 5.871030330657959, 5.890085220336914, 5.852896213531494, 5.909482002258301, 6.066002368927002, 5.854187488555908, 5.904773712158203, 5.81867790222168, 5.807529926300049, 5.818202972412109, 5.7391276359558105, 5.64459228515625, 5.50453519821167, 5.747126579284668, 5.689914703369141, 7.570035934448242, 5.776939392089844, 5.764033317565918, 5.759721279144287, 5.707263946533203, 5.730834484100342, 5.7165608406066895, 6.671480178833008, 5.706679344177246, 5.70188570022583, 5.616623878479004, 5.724441051483154, 5.762425899505615, 6.511679649353027, 5.694105625152588, 5.577602386474609, 5.433745384216309, 6.211427211761475, 7.359161376953125, 6.456727504730225, 5.74099588394165, 5.7338175773620605, 5.653929710388184, 5.669988632202148, 5.666616439819336, 5.692060947418213, 6.841648101806641, 5.641101837158203, 5.681558609008789, 5.686514854431152, 5.721410274505615, 5.67996072769165, 5.63045072555542, 5.709061622619629, 5.610651016235352, 5.706809043884277, 5.651412487030029, 5.604416370391846, 5.659921169281006, 5.6326751708984375, 5.591084957122803, 5.66522216796875, 5.604227542877197, 5.595333099365234, 5.56463098526001, 5.611722946166992, 5.5786871910095215, 5.5712151527404785, 5.5323615074157715, 5.508461952209473, 5.510690212249756, 5.538839817047119, 5.550665378570557, 5.584074974060059, 5.530534744262695, 5.546180725097656, 5.526200771331787, 5.470415115356445, 5.595020294189453, 5.47826623916626, 5.530332565307617, 5.475025177001953, 5.474307537078857, 5.495638370513916, 5.571838855743408, 5.502551555633545, 5.523373126983643, 5.512153148651123, 5.479115009307861, 5.5645527839660645, 5.506665229797363, 5.5253095626831055, 5.497145652770996, 5.537386894226074, 5.476159572601318, 5.462010860443115, 5.543686866760254, 5.509641170501709, 5.472754001617432, 5.441167831420898, 5.46715784072876, 31.981645584106445, 6.432806015014648, 5.4527106285095215, 5.477367877960205, 5.428871154785156, 5.463010787963867, 5.416673183441162, 5.444391250610352, 5.48887825012207, 5.395851135253906, 5.464864730834961, 5.4517621994018555, 5.391071319580078, 5.403730869293213, 5.410890579223633, 5.391526222229004, 5.401132583618164, 5.627889156341553, 5.709041595458984, 5.42781400680542, 5.370951175689697, 8.605616569519043, 5.369499683380127, 5.373633861541748, 5.348327159881592, 5.403226852416992, 5.375524997711182, 5.349157333374023, 5.364290714263916, 5.5444159507751465, 5.344534873962402, 5.387370586395264, 5.383146286010742, 5.32961368560791, 5.326359748840332, 5.323942184448242, 5.402902603149414, 5.366850852966309, 5.350793361663818, 5.344407081604004, 5.325777053833008, 5.343245983123779, 5.308211326599121, 5.3755364418029785, 5.341564178466797, 5.321069717407227, 5.332040309906006, 5.4312849044799805, 5.348320484161377, 5.35380220413208, 5.3565239906311035, 5.354175567626953, 5.324859619140625, 5.312098503112793, 5.31278133392334, 5.317283630371094, 5.371047019958496, 5.33012056350708, 5.29799747467041, 5.300539493560791, 5.347892761230469, 5.313864231109619, 5.810499668121338, 5.291258811950684, 5.3371663093566895, 5.299899101257324, 5.346197128295898, 5.339061737060547, 6.938959121704102, 7.381439208984375, 5.569159507751465, 5.330775737762451, 5.360335350036621, 5.326564788818359, 5.4394378662109375, 6.4164323806762695, 5.296302795410156, 7.225842475891113, 5.302742004394531, 5.301496982574463, 5.292601585388184, 5.280561447143555, 5.2621965408325195, 5.328785419464111, 5.295088768005371, 5.441130638122559, 5.263147354125977, 5.293435573577881, 5.271209239959717, 5.329047203063965, 5.349869251251221, 5.297714710235596, 5.280467510223389, 5.2750701904296875]\n"]}]},{"cell_type":"code","source":["fig, ax = plt.subplots()\n","ax.plot(train_accs, c=\"steelblue\", label=\"Training\")\n","ax.plot(val_accs, c=\"orangered\", label=\"Validation\")\n","# ax.plot(train_accs, c=\"steelblue\", label=\"Training\")\n","# ax.plot(val_accs, c=\"orangered\", label=\"Validation\")\n","ax.grid()\n","ax.legend()\n","ax.set_xlabel('Epoch')\n","ax.set_ylabel('Accuracy')\n","ax.legend(loc='best')\n","ax.set_title(\"Accuracy evolution\")\n","#plt.show()\n","plt.savefig(f\"Evolution_training_GNN_\" + name_dataset + \".png\")\n","\n","end = time.time()\n","time_to_train = (end - start)/60\n","print(\"Total training time to train on GPU (min):\", time_to_train)\n","print(\"=======End training process here======\")"],"metadata":{"id":"Cbt1cTEZ-gWt","colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1652892427428,"user_tz":-420,"elapsed":781,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}},"outputId":"c7443162-d78c-48aa-b010-4128e824d849"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Total training time to train on GPU (min): 23.438585154215495\n","=======End training process here======\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXhU5dn48e89k5Uk7BiEsCmI4AJIZJGqwa3gXpcKbS20KtVqrVbbSl9rKS7d7PpzeYuviksVt6posahI3BcQAdkNyBL2nYSsM3P//jhnkskwITPJDEkm9+e6cmXmnOeceZ6I555nF1XFGGOMCedp7gwYY4xpmSxAGGOMicgChDHGmIgsQBhjjInIAoQxxpiILEAYY4yJyAKEMUlARKaJyNNNuP4NEZkUzzyZ1i+luTNgTCgRKQSGAN1VtbKZs5OURGQa0F9Vvxc8pqrjmy9HpqWyGoRpMUSkL3A6oMDFR/iz7cuSMWEsQJiW5PvAJ8BMoE5zh4j0EpF/i8hOEdktIg+EnLtORFaKSImIrBCRU9zjKiL9Q9LNFJF73NcFIlIsIr8UkW3A4yLSSURedz9jr/s6L+T6ziLyuIhscc+/4h5fJiIXhaRLFZFdIjIsUiFF5EIRWSwi+0TkIxE52T3+SxF5MSzt30XkH+7rHiIyW0T2iEiRiFxXz/0LRKQ47Nh6ETlHRMYBvwKuEpFSEVnini8UkWvd1x4RuVNENojIDhF5UkQ6uOf6un/XSSKy0S3n/0TKh2n9LECYluT7wL/cn2+KSC6AiHiB14ENQF+gJzDLPXclMM29tj1OzWN3lJ/XHegM9AGm4Pz/8Lj7vjdQDjwQkv4poB1wAnAU8Ff3+JPA90LSnQ9sVdUvwj/QDRqPAT8CugD/BGaLSLpbpvNFJCek3N8GnnEvnwUUAz2AK4D7ROSsKMsKgKr+F7gPeE5Vs1V1SIRkk92fscAxQDZ1/w4A3wAGAmcDd4nIoFjyYVoHCxCmRRCRb+A8mJ9X1c+BtcB33NMjcB6KP1fVg6paoaofuOeuBf6oqgvUUaSqG6L82ADwG1WtVNVyVd2tqi+papmqlgD3Ame6+TsaGA9cr6p7VbVaVd917/M0zoO9vfv+apxgEskU4J+q+qmq+lX1CaASGOXmexHwLTftWUCZqn4iIr2AMcAv3fIvBv4PJzDG23eBv6jqOlUtBaYCE8Ka4X7r/s2WAEtw+o1MkrEAYVqKScCbqrrLff8Mtc1MvYANquqLcF0vnGDSGDtVtSL4RkTaicg/3aaVA8B7QEf3m3wvYI+q7g2/iapuAT4ELheRjjiB5F/1fGYf4Da3eWmfiOxz793DPf8MMNF9/R1qaw893M8vCbnXBpzaVLz1cO8d+jkpQG7IsW0hr8twahkmyVjHnGl2IpKJ05TidfsDANJxHs5DgE1AbxFJiRAkNgHH1nPrMpwmoaDuOE00QeFLGd+G02wyUlW3ichQ4AtA3M/pLCIdVXVfhM96Aqc2kwJ8rKqb68nTJuBeVb23nvMvAH92+z6+BYx2j29xPz8nJEj0BiJ9zkFCyu0GuG4h5xtawnkLTiAL6g34gO1AXsQrTFKyGoRpCS4F/MBgYKj7Mwh4H6cJ5TNgK/B7EckSkQwRGeNe+3/A7SIyXBz9RST4cFsMfEdEvG7n7JkN5CMHp99hn4h0Bn4TPKGqW4E3gIfczuxUETkj5NpXgFOAn+L0SdTnEeB6ERnp5jdLRC4I9juo6k6gEKcv5GtVXeke3wR8BPzOLf/JwDU4zVvh1gAZ7n1TgTtxAm7QdqCviNT3//+zwK0i0k9Esqnts4hUgzNJzAKEaQkmAY+r6kZV3Rb8wekY/S7ON/iLgP7ARpxawFUAqvoCTl/BM0AJzoO6s3vfn7rX7XPv80oD+fgbkAnswhlN9d+w81cD1cAqYAdwS/CEqpYDLwH9gH/X9wGquhC4zi3bXqAIp0M41DPAOdQ2LwVNxOmk3wK8jNN/8naEz9gP/BgneG7GqVGE1pxecH/vFpFFEbL5GE4fynvA10AF8JP6ymSSl9iGQcbEh4jcBRwXOgHNmNbM+iCMiQO3SeoanFqGMUnBmpiMaSJ3wtom4A1Vfa+582NMvFgTkzHGmIisBmGMMSaipOmD6Nq1q/bt27fR1x88eJCsrKz4ZagVsDK3DVbmtqGxZf788893qWq3SOeSJkD07duXhQsXNvr6wsJCCgoK4pehVsDK3DZYmduGxpZZROpdmiahTUwiMk5EVrsrT94R4XwfEZknIkvd1SRDV878o4gsF2eVzn+IiCQyr8YYY+pKWIBwp/c/iLMuzWBgoogMDkt2P/Ckqp4MTAd+5157Gs7CZCcDJwKn0vAsWGOMMXGUyBrECKDIXRGyCmep4kvC0gwG3nFfzw85r0AGkIazREAqzvIAxhhjjpBE9kH0xBkbHlQMjAxLswS4DPg7zsJkOSLSRVU/FpH5OOvvCPBAcE2aUCIyBWf5ZHJzcyksLGx0ZktLS5t0fWtkZW4brMxtQyLK3Nyd1LcDD4jIZJx1XzYDfnF2ARtE7cqRb4nI6ar6fujFqjoDmAGQn5+vTemUsk6ttsHK3DZYmeMjkQFiM84690F5hC1N7K6jfxmAu2rk5aq6z52Z+om7WQki8gbOssd1AoQxxpjESWQfxAJggLtkcBowAZgdmkBEuoYsOTwVZxVJcFbsPFNEUtzlis8EDmliMsYYkzgJq0Goqk9EbgLmAl7gMVVdLiLTgYWqOhsowFnfXnGamG50L38RZ7vFL3E6rP+rqq8lKq/GmPip8vl55bP1lFf56NAujYtP7YsnbJT6qs37+PSrxI072bC+ig2FqxN2/5amW/vMOjtjxUtC+yBUdQ4wJ+zYXSGvX8QJBuHX+XE2dTfGAKUV1azevK/BreAiWbfXR/banXHJR4d2aZRWVOMP6CHH+3dvT9G2A7y7fAsvfLyu5lxFlZ/+R3eok/4fc75k+75yEjW5SYGPNhUl6O4tz/E9O3LpMfG/b3N3UrcYW0v8bNxVSrf2GaR6PaR4bZmqlmTJ+t385/MNXJTfhxSvh5c//ZpAI56WO3dW8MGuSHvkNJ+AKiuL91JSXk1qiodjc9uzZut+AiEFrPYHmvQZzy37rKnZbJBHhIC7+OfJfTrz+++N4oYZ7/H4/Mjf5O+ZeCqn9j8qIXlpq53U8WYBAnjny83MXFzBzMXvAs63oSdvPouMVG8z56xtq/YH+GDlVvYdrOLVBevZureMom0H8HqEXQcq6No+I+Z7HjwYoIyShhMeYf27t6dPtxx2l1SwdvsBxp7Qg/bt0mrOez3CSb07k5ke+/+yixYt4pRTTmlyHlWVr3eU0Dk7nY5Z6XWOr9tewo795eR2zOSY3Pb06ZqN1yP86fuj2bzn4CH3ykz10i+3fZPzZBLLAgTwxde7APjJ6Xms3FXJ2yt3svtABT27tK3FvmLlDwTYd7CKLjl1H9QBVXYdqKCb+wDfsLOUdukpeD1Sk7a0ohoBsjJS61y7avM+Pl3jtE1/8fUuVm7eV3Pum0PzmLvY2Tnzl5cO5ayTesacZ+ebZdualL+jyMvgvE6RT6rCvh3O75RUyMiG0r3OucwcyKz7/8AJvTpHuEn9xzu0S6NDSKA7hN8PIuDxgK8aDuxusDzRSCvdA3u2gccLWR2gZE9c7gtAejsnv+WlTbuPeCCnU3zKnJLacJrG3DYhd21lNu4q5Qwt4sL7LuSsvBN4u98f2FdW2eoDhKqycVcpvbtmU99SVqrKhp2lVPn8/OW1pVRU+8lKT6F/9w6s3X6A3SUVZKR5qaoOkJHmpXN2Ov6AsnVvGRXVfsoqfUwqOI526Sls3FXKvoNVLN2wm5LyatpnpqJASXl1zed1zErDI8KBsiqAOt+SAfaXVeEPKB6BjNQUfnbRyYwemEuq10NmWgrXn3cCAO0a8U3aRPDUNHh6euRz2R3hqQ2QlcBv+pOPheNOhV+/AL8aB4vfafiaKJwG8I+43Kp1OH4kXPb7uN+2zf9fpn4/+zYXc9U25x9mu+Ll9MzdzH73AZYIAdVDRnXEm6ry2Duref6jtVyU34eT+3Thi693cUxuezJSvSzbtIf1myp4ae2HrNmyH3DakI/JzaG0oppPvtpO5+wMhh/bjXXbDtC7ayZb9hxk694yMlK9nNq/G16Ph7mLN/HCx+soq/SRlZ5CRpqX4cd0o1N2OhVVPgD65bantLyaA+VVVFb7AWr6eHxhbevt0lO4akz/er91tvnAEAgc/htnujuWpbKs5lDqwX2wr55O6vnPwoDhMP5a+McNzrHzp0DHo+CZe+DDf8OIC2rTt2sP3pRDv5FnZkN6JlSWR//Neucm2L7B+dmwApbMhzO/DUPGRnf9YaxZs4bjjjsOHrjR+ZudczUMPq3J96W6Eh6+xXk9+R5o36Xx93r0Dji4H868CoYUNC1fHY8CX9NuEUkb/78Ndm3ezBPvT3De9DoeNq1i1J5P2bDzLIb185GZFt8/kT+gfO/v8zi5TxemXjYsqmtKK6r506tLyOuSxXXnDGow/cGKal78eB3Pf7QWgNcWbuC1hXVX9M1I9ZKdGqBjijLxG/0BOG1gLsf16BhTeXz+AG8uKcbrER79cQGdstMbvsg03p9/CG89EdMlY8BZzKY+Nz0IF17v3HflJzDpbmjfGV5/GO7/Qd20PQdA70Hw8ey6xzt0hRnL4NpBULI3pvwBcJ1TM2Ti/8AxJ8d+fZgthYUcV1AAn78JH74M370LevZv8n2B2gAx8VdO81hjrfgY3n4SJkyFY4c0PV/WSR1/XY7qyoEf/oVNG9ZzwtU3E/jtZYza/Rk/n7+aNxZt5Mmbz4rr51VW+9lTWknh8i1cMfoYcjtkkpriYdrzC7lx3ImUlFfh8ytD+tZ+M1mzZT+fuO3yo47L5aTenXlvxVbWbT9A767ZnHnC0Xz61Q5SvR4+Wr2dOYs2AjBuWC9uGn8in67ZzoHyavp0y6ZTVjoKdM5O59OPPqCg4PQmladb+0wAenXJbnvBoboKKtwOWBGnrTvSA6OizPnm2VS+avjgJRh2Doy59NDzZSXw2FTn9bV/gAynibTm23Qkqelw1ned17+dDRuWQyd3ZNH016AoZMTX11/Cf/4Jm7+C0RfD8POc4zs2wvN/hH/e5gSHCVOha5T9Q517QNl+5+/YMTcuwaGO2x+HC66PX3AAeGw1VFU0LTgA/OQhKJgQn+CQIG0+QHgy2tF+wq3sLCyEHsfiGXURJzz7O9r5yti+H4p3l3L/7CXcfP5JHBOHURehwxVv+r8PALjryuEs/no31z38bs25F24/l/aZTjPLntKKmuP/88xneEUoq6qtT/7hlcWHfM5JvTtz0/gTSfV6+Mago5uc7/oERxJlZyamk+yI+2wO3HkBzCyCHsfWn85XDT8Y4Dwcg35wr/Ot8tL2kD8O7nweNq6E64c46ePlsltg5AWRzwUDxLd/UXOo5tt0Qzp2g44h6QaPdn6CDux2AkTw/ieMcV5XVznH3/mX0+QyabrTDNUSZHWA/PPie8+8eoJtrDKzYMT4+NwrQVrIf8UW5Lh8vAToVb6J1TkDeXNJMSs37eXDVdviEyB8h45nD7bLh3p7STGXjXJmvuwtdb59PnDtN3jsnVXs2FfO8R07Mu3b+by6YD3vLt/CBcP78PR7aygpr+bxG8c2aghoY2S5fQIJ6RuoqgBvKnjrGW5cWe60L6dlgK/KaQMP8lW752Ko1VRVwnN/cF6//yJc9cu651Wd2kBmFiz/0AkOF/3YeWA8ex+sX+akKSuB915wrnnrSfD74Lo/xWekSbv2cOphHir/XAqJmn7WvgvcM8cJFKHt+alpTu1j7RdOf0ZLCQ6myey/ZLheAwG4p/ghtpfDnVXTmPvp1TzvnQ5n/rrJtw/WIK4YfQwvurNNQ2sD3xrZj8/X7mTB2p01AWLPwUrSU730796e33237orp3z7tWL59mvNN99wheRwoO3TYaSL1OyoHgHMaMeT0sDZ/BT8cCMcOg4c+P/T8moVw8ygIuMHV44F/fAbHDXeCw6RjnQfZ42uia+7Yth6uG+wEHYAFbxwaIB75Bbx4P/zsUShe7Tzwr/k9tMtxOnsP7HaCQ9A7z8Bzv3dGmFx5e6P+DDHrd1Ji71/fN96Tz3B+TFKx6cLhjj4WPF7a71rLgINrGbXHmYF6/od/RLUxCx3UVe3z41E/x3VK4foz+pDur2D/wdoRUx6BU/t348sNe2omGO0traRzdnq9Q1WDUr0eJzioQvlB50FZXeW8Dv744zvUoXe3HF69YxxnntCj/kRVlXXzEE1+Vn3mlKNokVOTCLfi49rgAE5tYf4zzj0Xv+OMkKksc77Jh3yep6o8cj7ee94JDj+4zxnxsuwDOBAyUqeizGlCAVj+gVNb6D3YCQ7gfLvevwt2b6m9Jpj+1v9r+A9pTAtkNYhwqWnQvR9scdZxOX3XhwBk+0rZtr+c7h0buSTW7IfggRvxzSjmyc+uodsHzuS8bwELfdcx97NHuGvwr0mv6sakv4+kfPDt3Pyol/u/P6omQETt/34JL/wp8rncvjDzq/qbAT79D/z6QvjXJuiWFzlNmMPOOF+7GH4yov42+B7HwqOrD21GKg5ZnmFzEfQ78dDz7XJg4Aj4Yp7z3+ylvzg/4HS+tu8C/3ur8+M6A5yNbiPpeyJMnAorP4W3n3JqEWd/F958Au6fXJtu02rYt91pTglq39Xp4A0NEJ/NgW/99NC8G9NKWICIZOCpNQEif2/tKI6ir7fSfdhhOi4P59X/B0DKms/oVrWLPUMvYE/eUHL/+zdOXvQUAGftKKR09/EA3LzyL3zY6zzufHYBu0oqOGPw0c6sU3AepoEAaODQB73f7wxX7NG/pgxMutt5YG5d63QmLn0PhtUzOusN99vux6/CxTdGThNJdZXzjT/c/GedvF7ze2fmaKjNX8Ebj8Cy92HQ6LrnNq2qfb1huTO8Mvx83kD41SynNtFzAHwSsuBv3xOdALH03TqXrV23lmOPqee/4VD3bzLwVOiUCx+9AqdfAfOegtw+cMXtTtPWhy9DRSkUTKy9NliD2Lmp7j1HXRz5s4xpBSxARDJkLMx/lkDHXDz7apck3rK2CBobILr3g02rSCtaAMD+4ZewY8hFaOEsBpQ68xVKU7LptHc9AKIBHkx/i+/tdZaFGNw9y5l1enC/8437l2c7zRwAv33VeTgWr3GaVfbtcDpF/zTJOf/dO53f5aXw30eda2fWs9JlsCN10dvRB4iX/147NjySIWMPbc8HpyxvzYSf1zMx6qQznOBx34TI58/+njP+fvRFzvvexx+a5vgRdd5uKizk2IZG9Hg8MOoiJ1i+7y42/O1fwCU3wUt/rZ2H0Cvk8zp0dYZqhtY0AE5q2jBiY5qTBYhIxl0D6e0QFP5wdc1hCW0+iFX3fgBkfvUpANqhCxlpKexO68wAggEii757vq65pNv6z+jZ73w27y5l5MHlzoxTcB5QweAAMON259t40KBRzuzMvONqZ9aCM9v15ofhr9fBey8gR516aD43uvsyrf8y+rJtWOE091w1NfL50y6JfDyrA/z6pbplCTXqQqc5J7RsQSJw+uXR5zFW3/uNUwsLuLW08yY7x8+d5PR9eFPgtJC5CDkhM2qv/Dl06+VMKEvQGjnGHAkWICLxeODs7yKfv1XncOr+JmxwkuaMLOqw3FnSQ9p3JTPNy9a02gfLhOIXoRjnwTJoFMx9nIeqf82O6jR63P+Oc4/MbGckTajwB+jPn3CGdw4adWg+xl8Lrz0Ej03lTID1U+GH9znnnp5e+7De9rXTuZyWDq//LzxzLzyx1umjCVdZBh26Oe33sRp9UW0NIJJEj8qpT7e8yLWe9p0jj0jqGLJs9TW/d/4NGdPKJfRfsYiME5HVIlIkIndEON9HROaJyFIRKRSRvJBzvUXkTRFZKSIrRKRvIvMaUVbdTU4yDmzjsXmr2LG/PPZ7hXXSSoeuZLo1iFC7eg6Fm//X+fYKZKz8gN6bPoETT4dfv+jUDPbtcBL/apYzGesH98INf4NbZjjBoaGJPLc8ApPu5sDRA53aSCDg9B+8OdOpCfzgPufYVqdmwz9ugF3F8OV7ke9XWVa3ptIW5X8TpvwZ/viOBQeTNBJWgxARL/AgcC7O9+IFIjJbVVeEJLsfeFJVnxCRs4DfAcE2nSeBe1X1LRHJBpq2Y0pjZNeuS+TzpFC+dRPPfbSWom37uS9sPkJDtLqSAynt6eA7AIC3Y1dS07zsCQsQu3sNo+vJZ9QdxVNZ5kzJH3mB07Qx+0Hn+OmXN25S0nHD4bjhbN5dSvvX/wDjQkYQ3fy/zvnHf1W7Pk7QHefWf8+BI+o/1xZktIMrftbcuTAmrhLZxDQCKFLVdQAiMgu4BAgNEIOB4P9V84FX3LSDgRRVfQtAVZu48HojhdQgDmZ0IrPKmQTl8cQ+U7WivIIqT23zTGq7bDJSvaxoH9ax6nEf1udOcr6xz3vaee9O4GPYOU5toUvPJs9Y3Tn4LAZ17+yM8QenCeucq53fN/y9dsXOlFTo0qO2DyTUW0/A9vV1ZzEbY5JCIgNETyB0zF8xEP61ewlwGc5ak98CckSkC3AcsE9E/g30A94G7nD3qq4hIlOAKQC5ublN2nKvtLT0kOs9viqCc0PLvJlk+5yJa6X79sT8Wb02b6Gdp/ab+oLPPiEjRdjQrk+ddPsOHKi5d9oJ3+I0N0B8vGkPlfvdz+w0xKlPNXH1xtKKKgqPCvvm/4m7NWWnkyF8j5ne/Q65x0k5c+myfT27D1bwZQJWk4y3SP+dk52VuW1IRJmbu5P6duABEZkMvAdsBvw4+TodGAZsBJ4DJgOPhl6sqjOAGQD5+fnalD1o693D9o/OL39WJ7LclTv79e5JQUFsnadbXv8LPknlulMeolP1Xu48/RvkZKZy/0dzuOvE3zJ92W8A6Ni5CyND8+F9BAIBRl9wZSNKdXhx2bf3o75Q9Aldjs5rFXsAt9W9iq3MyS8RZU5kgNgM9Ap5n+ceq6GqW3BqELj9DJer6j4RKQYWhzRPvQKMIixAHEm+jByySrcBjVuYzl9VgU+8bMzqzUZ6k5riQUT4zbeHM6DDqfB9J0DgCZtRPP7apmY9sTLdpSasicmYpJPI4RYLgAEi0k9E0oAJQJ1dRkSkq0jN9NqpwGMh13YUkW7u+7Oo23dxxPky25PlNjGlpcT+ZxNfNT5PbWBJdXdUO21gd7p1qN3aVFrbCJjMbOd3Wx/FZEwSStjTSFV9wE3AXGAl8LyqLheR6SISXH+gAFgtImuAXOBe91o/TvPTPBH5Emf94kcSlddo+DPac1TlTua+fyE91sS+b674q/GJEyAE8IZ2dIeuQ+Rp7la/GAUXq0s7civIGmOOjIQ+jVR1DjAn7NhdIa9fBF6s59q3gDhvL9UIf/0APF4CL8wkVZ2VR0/85FGYckNMtxFfSIAQqbsya8gaRVLf3gctVbCJSY/8KGRjTGK1sq+rzSC4a1bWyzWHqlJjb04Rf20TUyB8UbvQfofW1sQU7HuI8zLixpjm18qeRs3nYEptP0FVSuwdsp6QJqZDT4b8ZwjvpG7pgvkNWA3CmGRjASJK6R1r10xqdA2ivgAR0tzU6pqYagLEodumGmNaNwsQUTrx5NoZz5WNrkE4D9PDbrATvmdCS2cBwpik1cqeRs1HutTua1yZEnsNwhPw4fM4Sz/ffvGQ+j+ntdUg2rWv+9sYkzSskzpaXWr3XK7yxL7Gv8dfTXpGBq9NHUdaSv1BQFrbMNdvXOasYnrBlObOiTEmzlrZ06gZBcf7Q6OaUzwBH4GU1MMGBwDxtrJKncdjq5gak6Ra2dOoGYXOW/DHHiC8/mo0mtpBaxvFZIxJWhYgGiMQ+5h/T8BHwNtw05Q0cQlvY4yJFwsQsfijs8SGNqKJyRuoRqMIEK1uopwxJmnZ0ygWQ8dSkdIOaUwTU8CHRrGBfasbxWSMSVoWIGKk4om9iSkQwKv+qGoQHuuDMMa0EBYgYqQeDxLrshLP/d65NiWtgYTUXdnVGGOakQWIGAUkJbZhroEAPP4/AKT7KxtMLlaDMMa0EBYgYqQeT2wrlxYtqnnZqXRLg8ltFJMxpqVIaIAQkXEislpEikTkjgjn+4jIPBFZKiKFIpIXdr69iBSLyAOJzGcsVLx4NIYaxI6NAHza/Qw+HH1Tg8lb3Y5yxpiklbCnkYh4gQeB8cBgYKKIDA5Ldj/wpKqeDEwHfhd2/m7gvUTlsTECHi8SSxNTRRkAzw74AeWdejWQ2GoQxpiWI5FfV0cARaq6TlWrgFnAJWFpBgPB/Tvnh54XkeE425C+mcA8xkw93tj6IKrKASiT1LrbjNbDY53UxpgWIpEBoiewKeR9sXss1BLgMvf1t4AcEekiIh7gzzj7Urco6vHGNoqp0qlBlJNOahTrLFkTkzGmpWju9ozbgQdEZDJOU9JmwA/8GJijqsV19m4OIyJTgCkAubm5FBYWNjojpaWlUV0/yB8Af1XUn9V75TKOAQ4EvGzZXExh4Y6I6Qrc36uL1rIiNbp7N1W0ZU4mVua2wcocH4kMEJuB0Eb3PPdYDVXdgluDEJFs4HJV3Scio4HTReTHQDaQJiKlqnpH2PUzgBkA+fn5WlBQ0OjMFhYWEs31u2dk4IGo0gKw4R0QoUpS6de3DwUFAyOnu8/5NWjwYLqeGeW9myjaMicTK3PbYGWOj0QGiAXAABHphxMYJgDfCU0gIl2BPaoaAKYCjwGo6ndD0kwG8sODQ3NRjze2pTYqy9C0TAIIKVE0MdlMamNMS5GwBm9V9QE3AXOBlcDzqrpcRKaLyMVusgJgtYiswemQvjdR+YkX9XjxxDqKKd3ZgS4lik5qSWnuVj9jjHEk9GmkqnOAOWHH7gp5/SLwYgP3mAnMTED2GsfjRdSPqnK4/pEaVeVoMEBE00lto5iMMS2EDZmJkXq8eAkQ0CgvqCzDl5IOQHpqFE1MrW1HOWNM0rKnUYzUE5xJHWWEqChjZ6WQk5nKN44/usHkrW5PamNM0rIAESP1ePFq9DWIspIS9vpS+M43+tMpO73B9NYHYVXgSFQAACAASURBVIxpKSxAxMqtQahGFyF8ZQep8qQxpG+XqNJ7baKcMaaFsKdRjII1iCjjA56qMio96VF1UIPVIIwxLYcFiBipx4uHQLQ9EEh1BRXe6AOExxbrM8a0EBYgYqTidbYPjbIK4akqp8qTFtU6TGDDXI0xLYcFiBg5o5hiaGKqrqRaUqMOEFaDMMa0FBYgYqReN0BE2cgkgWqqPSkxNDFZDcIY0zJYgIhVTRNTdMk9fh9+SSHVG8Wsa2xPamNMy2EBIkY1ndRRBgjx+/CJN+oaBFaDMMa0EBYgYlQ7zDWKCKGKN1CNX1Ki2k0OAKtBGGNaCAsQsQpOlIsmrbvqa8CbGt3CfgBi/0mMMS2DPY1ipJ4UdxRTFCGiusq5xpsa/QdYDcIY00JYgIhRTDOp/dXONbHMjrY+CGNMC2EBIlYeLx78BKKJED43QMRSg7AmJmNMC5HQp5GIjBOR1SJSJCKHbBkqIn1EZJ6ILBWRQhHJc48PFZGPRWS5e+6qROYzFsGJclFpTICwJiZjTAuRsAAhIl7gQWA8MBiYKCKDw5LdDzypqicD04HfucfLgO+r6gnAOOBvItIxUXmNiTf2JiYsQBhjWqFE1iBGAEWquk5Vq4BZwCVhaQYD77iv5wfPq+oaVf3Kfb0F2AF0S2BeoxbcMCiWJiZSYgkQ1sRkjGkZErnwT09gU8j7YmBkWJolwGXA34FvATki0kVVdwcTiMgIIA1YG/4BIjIFmAKQm5tLYWFhozNbWloa1fXt9u6jrwb4+OOP6ZBx+Id5u10bGAGUV/sbvHeB+/vdDz6IrUmqCaItczKxMrcNVub4aO6V4W4HHhCRycB7wGbAHzwpIkcDTwGTVA9t+FfVGcAMgPz8fC0oKGh0RgoLC4nm+qLPXsFLgJEjR9K9U9bhE69bCkBGTk7D977P+XVmwVlHbCRTtGVOJlbmtsHKHB+JDBCbgV4h7/PcYzXc5qPLAEQkG7hcVfe579sD/wH+R1U/SWA+Y1KzZ3Qgio5qt4lJYuqDsCYmY0zLkMin0QJggIj0E5E0YAIwOzSBiHQVqRnXORV4zD2eBryM04H9YgLzGLNgz0MgmgAR7KROTYv+A6KdcW2MMQmWsAChqj7gJmAusBJ4XlWXi8h0EbnYTVYArBaRNUAucK97/NvAGcBkEVns/gxNVF5jIe6aSlHNpA7WIGLppDbGmBYioX0QqjoHmBN27K6Q1y8Ch9QQVPVp4OlE5q3RxOkfiNAlcqiaUUwx1CCMMaaFsAbvWLlNQOqPJkA4azF5oqlBTH0Ghp/XlJwZY0xcNfcoplandlXWGJqYoumDGDvR+THGmBbCahAxUrdPXWPopI4qQBhjTAtjASJGNZ3UMQxz9VqAMMa0Qg0GCBG5KGQoqiH2AOFJswBhjGl9onnwXwV8JSJ/FJHjE52hFs+dyKZR9EGo20ltNQhjTGvUYIBQ1e8Bw3DWQprpLsM9RURyEp67Fqimk9rvP3xCIFBdCYDXahDGmFYoqqYjVT2AM19hFnA0zsJ6i0TkJwnMWwsVnCjXcEp/lVuDSMtIZIaMMSYhoumDuFhEXgYKgVRghKqOB4YAtyU2ey1QsIkpiggRDBAp6VaDMMa0PtHMg7gc+Kuqvhd6UFXLROSaxGSrBQtOlKtvJvXGVbD+Szj9CtJe+QsAKWnpRyp3xhgTN9EEiGnA1uAbEckEclV1varOS1TGWqqaPoiQUUx3v/A5H6zaxtxfXwDXDnIO/nMp3r3bAEhNtwBhjGl9oumDeAEI/brsd4+1TcGJciE1iA9WbTs03d7tNS9TrJPaGNMKRRMgUtwtQwFwX7fdJ16wDyLQQB/EHqfStS+lPakpts+0Mab1iSZA7AxZnhsRuQTYlbgstWzBJiYNNDDMdeadAEwa8ShpFiCMMa1QNH0Q1wP/EpEHcMZ4bgK+n9BctWTBPoiGRjHt2AhAhTeTtBSbiG6MaX0aDBCquhYY5W4JiqqWJjxXLZm7H0QgmokQLgsQxpjWKKonl4hcAPwY+JmI3CUidzV0jXvdOBFZLSJFInJHhPN9RGSeiCwVkUIRyQs5N0lEvnJ/JkVboEQLrkolUazFtHvwOQDWxGSMaZWimSj3vzjrMf0Ep4npSqBPFNd5gQeB8cBgYKKIDA5Ldj/OvtMnA9OB37nXdgZ+A4wERgC/EZFOUZYpsST6LUeXX/knAFKtBmGMaYWieXKdpqrfB/aq6m+B0cBxUVw3AihS1XXuyKdZwCVhaQYD77iv54ec/ybwlqruUdW9wFvAuCg+M/FqmpgarkFUummtickY0xpF00ld4f4uE5EewG6c9Zga0hOnQzuoGKdGEGoJcBnwd5z1nXJEpEs91/YM/wARmQJMAcjNzaWwsDCKbEVWWloa1fWpmzbRG1i5fDk7S/YD0KN8M90qd1NYmEVBSNoVX30NeFnwySe0S5MId2te0ZY5mViZ2wYrc3xEEyBeE5GOwJ+ARTh7bT4Sp8+/HXhARCYD7wGbcSbiRUVVZwAzAPLz87WgoKDRGSksLCSa6zdtXQ7A8QOP58SR+QAU3DfWOflnhftq0+YdOxA2FlFw5um0S295u7tGW+ZkYmVuG6zM8XHYp5a7UdA8Vd0HvCQirwMZqro/intvBnqFvM9zj9VQ1S04NQjcUVKXq+o+EdkMdb6M5+EsFtjspGY/iCiamAJOrcGamIwxrdFhn1zqrCfxYMj7yiiDA8ACYICI9BORNGACMDs0gYh0DdmtbirwmPt6LnCeiHRyO6fPc481v5q1mBrupK6oDuARwetpec1LxhjTkGi+2s4TkculZpW66KiqD7gJ58G+EnheVZeLyPSQmdkFwGoRWQPkAve61+4B7sYJMguA6e6xZheMZ9FsObp+Zwl5XbKI8U9njDEtQjQN4z8Cfgb4RKQCZ6irqmr7hi5U1TnAnLBjd4W8fhFnI6JI1z5GbY2ixZBoZ1IDRdv2M7Rv1wTnyBhjEiOamdRtcmvR+mjNhkGH1iBUldC6wu6SSvp3bzCOGmNMi9RggBCRMyIdD99AqK2ItB9EkALhjUld22cmPE/GGJMI0TQx/TzkdQbOBLjPgbMSkqMWrqYPIkITU6RjLXF4qzHGRCOaJqaLQt+LSC/gbwnLUUvnNjFF6oOI1C1hAcIY01o1ZoB+MTAo3hlpLWr6qCM0MUVa4bVdmgUIY0zrFE0fxP/DaV4HJ6AMxZlR3TYFp21E7KQ+NLnVIIwxrVU0T6+FIa99wLOq+mGC8tPi1c6DiK4PIssChDGmlYrm6fUiUKGqfnCW8RaRdqpaltistVCe+msQAQU/Hrwhy3BkWBOTMaaVimomNRA6VjMTeDsx2Wn5gsNcg/0Nof0OkWoQtsyGMaa1iiZAZIRuM+q+bpe4LLVwboAQNxj4Q5qaAgoBsYX5jDHJIZqn2UEROSX4RkSGA+WJy1LLVrOaa8BZlTw0QKgq0e9UbYwxLVs0DeS3AC+IyBacicLdcbYgbZPC12Ly+2v7GwKqqHiwKGGMSQbRTJRbICLHAwPdQ6tVtTqx2WrBPM42ohqhiUkVQldjuv/7o45s3owxJo4abGISkRuBLFVdpqrLgGwR+XHis9YyBWsQwYlydfogAoE6AeKkPl2ObOaMMSaOoumDuM7dUQ4AVd0LXJe4LLVs4U1MvpAZ1RpQArb3gzEmSUQTILyhmwWJiBdIS1yWWjh32Gpwue/wGoSFB2NMsogmQPwXeE5EzhaRs4FngTeiubmIjBOR1SJSJCJ3RDjfW0Tmi8gXIrJURM53j6eKyBMi8qWIrBSRqbEUKpHE7YMI9kT7/SF9EAE/nggT6IwxpjWKJkD8EngHuN79+ZK6E+cicmsaDwLjgcHARBEZHJbsTpytSIfh7Fn9kHv8SiBdVU8ChgM/EpG+UeT1CKi7H4S/ThNTAK8z4Zz53c484jkzxph4ajBAqNOW8imwHmcviLNw9phuyAigSFXXqWoVMAu4JPz2QHDLtQ7AlpDjWSKSghOMqoADUXxmwkmwiSkQYaJcIIBHA7zQ8zL+MPC2ZsmfMcbES73DXEXkOGCi+7MLeA5AVcdGee+ewKaQ98XAyLA004A3ReQnQBZwjnv8RZxgshVn1vatqronQh6nAFMAcnNzKSwsjDJrhyotLY3q+rSilZwGbNq0kYrCQraV+unnnlvw2adcQoBKbzoqnibl50iItszJxMrcNliZ4+Nw8yBWAe8DF6pqEYCI3BrXT3eCz0xV/bOIjAaeEpETcWoffqAH0Al4X0TeVtV1oRer6gxgBkB+fr4WFBQ0OiOFhYVEc/3+DB88D3k9ezK6oIA/vbq45tzwoUMBCLgVs6bk50iItszJxMrcNliZ4+NwTUyX4XyDny8ij7gd1LEM0tkM9Ap5n+ceC3UN8DyAqn6Ms6VpV+A7wH9VtVpVdwAfAvkxfHbCeDx1h7m+vTSkSL4qwNZjMsYkh3qfZKr6iqpOAI4H5uMsuXGUiDwsIudFce8FwAAR6SciaTid0LPD0mwEzgYQkUE4AWKne/ws93gWMAqnRtP8Qpb7VlVSJWQUk8/nvPCm8I9rxjRD5owxJn6i6aQ+qKrPuHtT5wFf4Ixsaug6H3ATMBenU/t5VV0uItNF5GI32W3AdSKyBGf47GR11rB4EGfG9nKcQPO4qi5tRPnirmaxPg1QUlGN+n015z5ZVQzASf2OYmCPjs2SP2OMiZeYdrNxZ1HXtPtHkX4OMCfs2F0hr1cAh3zVdpcUvzKWvB0pUjPMVdlTUlkzrBXg9U/WciUgXm/ki40xphWxxvJY1TQxKXsPVtaZGJeiTm2idjKdMca0XhYgYhRsYiIQYG9pJd6QAOHVYB+EBQhjTOtnASJGNau5ouwrq6rTxBQMFuK1faiNMa2fBYhYhTQxVfsCeKkNEMEmJqyJyRiTBCxAxKhmYduAH7+7tEZQilubqFZb09UY0/pZgIhR7TBXCAS0bhNTwKlBVAYsQBhjWj8LEDGqGeaqAXwBJYVDaxCVtuK3MSYJWICIkXjdGkQgQJXPX2cmtbcmQFgNwhjT+tlwm1i56ywVLtvMB9u/5jiprS609zkrkud0aB/xUmOMaU2sBhGjYCd1sI6QGhIgRuxZiF88nHb5Fc2QM2OMiS8LEDEKzpIWd8vR0Camk/ctZUf3k/B26NIseTPGmHiyABGj4I5y4g5vTQ3ppM4IVFKVYc1LxpjkYAEiRsFRTMEmpkyqa8551Y/aJDljTJKwABGj4EqtwSam7uVba8551V/TiW2MMa2dPc1iVdNJ7QSIHgdrt922GoQxJpkkNECIyDgRWS0iRSJyR4TzvUVkvoh8ISJLReT8kHMni8jHIrJcRL4UkYxE5jV6wT4IJ0AcXVobIDyo1SCMMUkjYfMgRMSLszPcuUAxsEBEZrubBAXdibPT3MMiMhhnc6G+IpICPA1crapLRKQLhDT2Nyd3qY1gDaLbwS1h560GYYxJDon8ujsCKFLVdapaBcwCLglLo0Bw2E8HIPi0PQ9YqqpLAFR1t2rIokfNyqlBpAWqyPCXk1ldir9dh9rTHqtBGGOSg6hqw6kac2ORK4Bxqnqt+/5qYKSq3hSS5mjgTaATkAWco6qfi8gtwHDgKKAbMEtV/xjhM6YAUwByc3OHz5o1q9H5LS0tJTs7u8F0Gfu2MeqhiVR40lmdM4D+B7+GzCyySnYA8GW/s9k98c5G5+NIirbMycTK3DZYmaM3duzYz1U1P9K55l5qYyIwU1X/LCKjgadE5EQ3X98ATgXKgHki8rmqzgu9WFVr9sfOz8/XgoKCRmeksLCQqK7fsREecuY8dKnaQ6avHF+HPuAGiOyc9pzUhHwcSVGXOYlYmdsGK3N8JLI9ZDPQK+R9nnss1DXA8wCq+jGQAXTF6bN4T1V3qWoZTt/EKQnMawxqF+LL9FfgIUCgXe3kOBvFZIxJFokMEAuAASLST0TSgAnA7LA0G4GzAURkEE6A2AnMBU4SkXZuh/WZwApaAqkNENm+UgA0JEDgsZVcjTHJIWFNTKrqE5GbcB72XuAxVV0uItOBhao6G7gNeEREbsXpsJ6sTqfIXhH5C06QUWCOqv4nUXmNSUgndHqgCqgbIMRqEMaYJJHQPghVnYPTPBR67K6Q1yuAMfVc+zTOUNcW5tAagtooJmNMErKnWawkUoDICTlvNQhjTHKwABGrCAGCOjUICxDGmORgASJWEZbSqNtJbQHCGJMcLEDEqqEahNcChDEmOViAiFUDfRBii/UZY5KEPc1iFSEASFpm7RurQRhjkoQFiFhFqEFIenrtGxvmaoxJEvY0i1WkAJGSVvvaOqmNMUnCAkSsIvUxhAYFq0EYY5KEPc1iFaEG4Qk5ZjUIY0yysAARswhNTN6QP6MFCGNMkrAAEasITUgSckxsFJMxJklYgIhVpE7qkH4JsT4IY0ySsKdZzBpqYmruTfqMMSY+LEDEKqyGoCJWgzDGJCV7msUqrIlJROqMXLI+CGNMskhogBCRcSKyWkSKROSOCOd7i8h8EflCRJaKyPkRzpeKyO2JzGdswpqYxIPHazUIY0zySdjTTES8wIPAeGAwMFFEBocluxN4XlWH4exZ/VDY+b8AbyQqj40S3kkd1sSE1/ogjDHJIZFfd0cARaq6TlWrgFnAJWFpFAhuptAB2BI8ISKXAl8DyxOYx9gdEiA8dfolrAZhjEkWify62xPYFPK+GBgZlmYa8KaI/ATIAs4BEJFs4JfAuUC9zUsiMgWYApCbm0thYWGjM1taWhr19QUhr/2qLFywoKZgGzcVs6oJ+TiSYilzsrAytw1W5vho7vaQicBMVf2ziIwGnhKRE3ECx19VtVQibdDjUtUZwAyA/Px8LSgoaHRGCgsLifr6+2pfer1eRo4cBf903vftdwxHNSEfR1JMZU4SVua2wcocH4kMEJuBXiHv89xjoa4BxgGo6scikgF0xalpXCEifwQ6AgERqVDVBxKY30aSOgv42SgmY+Kjurqa4uJiKioqYr62Q4cOrFy5MgG5arkaKnNGRgZ5eXmkpqZGfc9EBogFwAAR6YcTGCYA3wlLsxE4G5gpIoOADGCnqp4eTCAi04DSlhkccPok6izWZ30QxsRDcXExOTk59O3bl8O1JERSUlJCTk5OwwmTyOHKrKrs3r2b4uJi+vXrF/U9E/Y0U1UfcBMwF1iJM1ppuYhMF5GL3WS3AdeJyBLgWWCyqmqi8pQQElaDSGnuVjtjkkNFRQVdunSJOTiYQ4kIXbp0ibk2ltCnmarOAeaEHbsr5PUKYEwD95iWkMzFi41iMiZhLDjET2P+lvY0ayqPp04Tk4r1QRhjkoMFiKYKa2KqDrSuFjJjTGS7d+9m6NChDB06lO7du9OzZ8+a91VVVYe9duHChdx8880NfsZpp50Wr+wmhDWYN1ndTupunbKbMS/GmHjp0qULixcvBmDatGlkZ2dz++2107J8Ph8p9fQ55ufnk5+f3+BnfPTRR/HJbIJYgGiqsBqE15baMCbuHp67nHXbD0Sd3ufzk5Jy+ObeY3Lbc8M3T4gpH5MnTyYjI4MvvviCMWPGMGHCBH76059SUVFBZmYmjz/+OAMHDqSwsJD777+f119/nWnTprFx40bWrVvHxo0bueWWW2pqF9nZ2TUT3KZNm0bXrl1ZtmwZw4cP5+mnn0ZEmDNnDj/72c/IyspizJgxrFu3jtdffz2mfDeWPc2aSqTuEuC25agxSa24uJiPPvoIr9fLgQMHeP/990lJSeHtt9/mV7/6FS+99NIh16xatYr58+dTUlLCwIEDueGGGw6Zj/DFF1+wfPlyevTowZgxY/jwww/Jz8/nRz/6Ee+99x79+vVj4sSJR6qYgAWIphMPdVZ4FevWMSbeYv2mn8h5EFdeeSVed0Ls/v37mTRpEl999RUiQnV1dcRrLrjgAtLT00lPT+eoo45i+/bt5OXl1UkzYsSImmNDhw5l/fr1ZGdnc8wxx9TMXZg4cSIzZsxISLkisadZU1kNwpg2JSsrq+b1r3/9a8aOHcuyZct47bXX6p1nkJ6eXvPa6/Xi8/kaleZIswDRVOKpW2uweRDGtBn79++nZ8+eAMycOTPu9x84cCDr1q1j/fr1ADz33HNx/4zDsadZU4UttWE1CGPajl/84hdMnTqVYcOGJeQbf2ZmJg899BDjxo1j+PDh5OTk0KFDh7h/Tn2sD6KpOnUPa2KymGtMspk2bVrE46NHj2bNmjU17++55x4ACgoKalZWDb922bJlNa9LS0sPSQ/wwAO1S8+NHTuWVatWoarceOONUQ2fjRd7mjXFrY/A3a9Rp5PaahDGmDh65JFHGDp0KCeccAL79+/nRz/60RH7bKtBNMX4a53f5aW1x2wUkzEmjm699VZuvfXWZvlse5rFg9goJmNM8rEAEQ91OqntT2qMSQ72NIsHq0EYY5KQBYh4sBqEMSYJJfRpJiLjRGS1iBSJyB0RzvcWkfki8oWILBWR893j54rI5yLypfv7rETmM2a/mgUPLKh9bzOpjUk6Y8eOZe7cuXWO/e1vf+OGG26ImL6goICFCxcCcP7557Nv375D0kybNo3777//sJ/7yiuvsGLFipr3d911F2+//Xas2Y+LhAUIEfECDwLjgcHARBEZHJbsTpytSIfh7Fn9kHt8F3CRqp4ETAKeSlQ+G6XgKjguZCxyaBOTjWIyJilMnDiRWbNm1Tk2a9asqBbMmzNnDh07dmzU54YHiOnTp3POOec06l5NlchhriOAIlVdByAis4BLgBUhaRRo777uAGwBUNUvQtIsBzJFJF1VKxOY38azGoQxifXwLbB2cdTJM/1+8Dbw/+KxQ+GGv9V7+oorruDOO++kqqqKtLQ01q9fz5YtW3j22Wf52c9+Rnl5OVdccQW//e1vD7m2b9++LFy4kK5du3LvvffyxBNPcNRRR9GrVy+GDx8OOPMbZsyYQVVVFf379+epp55i8eLFzJ49m3fffZd77rmHl156ibvvvpsLL7yQK664gnnz5nH77bfj8/k49dRTefjhh0lPT6dv375MmDCBN998k+rqal544QWOP/74qP9e9UlkgOgJbAp5XwyMDEszDXhTRH4CZAGRwuTlwKJIwUFEpgBTAHJzcyksLGx0ZoNrsjdWgfv70wULKF+3rdH3OZKaWubWyMrcenTo0IGSkhIA0quq8Pj90V+siq+B9IGqKird+0eSmprKKaecwr///W8uuOACnnjiCS699FJuu+02OnfujN/v56KLLmLcuHGceOKJ+P1+Dh48SElJCapKaWkpK1as4JlnnuH999/H5/Nx+umnc+KJJ1JSUsK5557LhAkTAKeW8OCDD3L99dczfvx4xo0bx6WXXgpAdXU15eXl7Ny5k0mTJjF79mwGDBjAlClT+Otf/8qNN96IqtK5c2feffddHnnkEX73u9/VmY0dVFFREdO/heaeKDcRmKmqfxaR0cBTInKiqgYAROQE4A/AeZEuVtUZwAyA/Px8DZ2qHqvCwkKacj33Ob9Gjj4Nehzb+PscQU0ucytkZW49Vq5cWbtk908fOnziMNEu953WwPmrr76aV199lQkTJvDyyy/z6KOP8sYbbzBjxgx8Ph9bt25lw4YNjB49Gq/XS1ZWFjk5OYgI2dnZLFq0iMsvv5zc3FwALr30UtLT08nJyWHRokVcffXV7Nu3j9LSUr75zW+Sk5NDamoqmZmZNfkPvt+yZQvHHHMMp5xyCgDXXnstDz74IHfccQciwiWXXEJOTg5jxoxhzpw5EcufkZHBsGHDov47JrLBfDPQK+R9nnss1DXA8wCq+jGQAXQFEJE84GXg+6q6NoH5jC/rgzAmaVxyySXMmzePRYsWUVZWRufOnbn//vuZN28eS5cu5YILLqh3ie+GTJ48mQceeIAvv/yS3/zmN42+T1BwufB4LhWeyKfZAmCAiPQTkTScTujZYWk2AmcDiMggnACxU0Q6Av8B7lDVDxOYx/jJ6ez8tmGuxiSN7Oxsxo4dyw9/+EMmTpzIgQMHyMrKokOHDmzfvp033njjsNefccYZvPLKK5SXl1NSUsJrr71Wc66kpISjjz6a6upq/vWvf9Ucz8nJqWlaCzVw4EDWr19PUVERAE899RRnnnlmnEoaWcKeZqrqA24C5gIrcUYrLReR6SJysZvsNuA6EVkCPAtMVlV1r+sP3CUii92foxKV17g4+hjnd2VZ8+bDGBNXEydOZMmSJUycOJEhQ4YwbNgwjj/+eL7zne8wZsyYw157yimncNVVVzFkyBDGjx/PqaeeWnPu7rvvZuTIkYwZM6ZOh/KECRP405/+xLBhw1i7trbxJCMjg8cff5wrr7ySk046CY/Hw/XXXx//AocQ53nc+uXn52twDHJjNLmddmcx/OefMGl63YlzLVhrbZtuCitz67Fy5UoGDRrUqGsTueVoSxVNmSP9TUXkc1WNuIZ4c3dSJ49ueTD57ubOhTHGxI01mBtjjInIAoQxpsVKlibwlqAxf0sLEMaYFikjI4Pdu3dbkIgDVWX37t1kZGTEdJ31QRhjWqS8vDyKi4vZuXNnzNdWVFTE/DBs7Roqc0ZGBnl5eTHd0wKEMaZFSk1NpV+/fo26trCwMKYZw8kgEWW2JiZjjDERWYAwxhgTkQUIY4wxESXNTGoR2QlsaMItuuJsVNSWWJnbBitz29DYMvdR1W6RTiRNgGgqEVlY33TzZGVlbhuszG1DIspsTUzGGGMisgBhjDEmIgsQtWY0dwaagZW5bbAytw1xL7P1QRhjjInIahDGGGMisgBhjDEmojYfIERknIisFpEiEbmjufMTLyLymIjsEJFlIcc6i8hbIvKV+7uTe1xE5B/u32CpiJzSfDlvPBHpJSLzRWSFiCwXkZ+6x5O23CKSISKficgSt8y/dY/3E5FP3bI95+4Lj4iku++L3PN9mzP/TSEiXhH5SrddaQAABMhJREFUQkRed98ndZlFZL2IfOluwbzQPZbQf9ttOkCIiBd4EBgPDAYmisjg5s1V3MwExoUduwOYp6oDgHnue3DKP8D9mQI8fITyGG8+4DZVHQyMAm50/3smc7krgbNUdQgwFBgnIqOAPwB/VdX+wF7gGjf9NcBe9/hf3XSt1U9x9rsPagtlHquqQ0PmOyT237aqttkfYDQwN+T9VGBqc+crjuXrCywLeb8aONp9fTSw2n39T2BipHSt+Qd4FTi3rZQbaAcsAkbizKhNcY/X/DsH5gKj3dcpbjpp7rw3oqx57gPxLOB1QNpAmdcDXcOOJfTfdpuuQQA9gU0h74vdY8kqV1W3uq+3Abnu66T7O7jNCMOAT0nycrtNLYuBHcBbwFpgn6r63CSh5aops3t+P9DlyOY4Lv4G/AIIuO+7kPxlVuBNEflcRKa4xxL6b9v2g2ijVFVFJCnHOItINvAScIuqHhCRmnPJWG5V9QNDRaQj8DJwfDNnKaFE5EJgh6p+LiIFzZ2fI+gbqrpZRI4C3hKRVaEnE/Fvu63XIDYDvULe57nHktV2ETkawP29wz2eNH8HEUnFCQ7/UtV/u4eTvtwAqroPmI/TvNJRRIJfAEPLVVNm93wHYPcRzmpTjQEuFpH1wCycZqa/k9xlRlU3u7934HwRGEGC/2239QCxABjgjn5IAyYAs5s5T4k0G5jkvp6E00YfPP59d+TDKGB/SLW11RCnqvAosFJV/xJyKmnLLSLd3JoDIpKJ0+eyEidQXOEmCy9z8G9xBfCOuo3UrYWqTlXVPFXti/P/7Duq+l2SuMwikiUiOcHXwHnAMhL9b7u5O16a+wc4H1iD0277/9u7f9CogiAA498gogFB/AM2IkewE4NIKrGwtrUIYiWpUoiVWAhWVpZRGy3EQiwsbCxEjSCCQhpN1MoodhGSQkGQEMJY7J4c+gKeXvIkfj943MtcOHbguLndd2/2QtvjGWBed4B5YJmy/jhOWXedAt4Bj4Gd9X+D8muu98BrYLTt8f9hzkcp67SzwKt6HN/IeQMjwMua8xvgYo0PA9PAHHAX2FLjW+vfc/X54bZz+Mv8jwH3N3rONbeZerztflat9XvbVhuSpEb/+xKTJGkVFghJUiMLhCSpkQVCktTIAiFJamSBkPoQESu1m2b3GFgH4IjoRE/3XaltttqQ+vMtMw+1PQhpPTiDkAag9uq/XPv1T0fE/hrvRMST2pN/KiL21fieiLhX93GYiYgj9aU2RcSNurfDw3p3tNQKC4TUn6GflpjGep77kpkHgauUbqMAV4BbmTkC3AYma3wSeJplH4fDlLtjofTvv5aZB4DPwIk1zkdalXdSS32IiK+Zua0h/pGycc+H2jDwU2buiohFSh/+5Rqfz8zdEbEA7M3MpZ7X6ACPsmz+QkScBzZn5qW1z0z6lTMIaXBylfN+LPWcr+B1QrXIAiENzljP44t6/pzScRTgFPCsnk8BE/Bjw5/t6zVI6Xf57UTqz1Ddva3rQWZ2f+q6IyJmKbOAkzV2BrgZEeeABeB0jZ8FrkfEOGWmMEHpviv9M7wGIQ1AvQYxmpmLbY9FGhSXmCRJjZxBSJIaOYOQJDWyQEiSGlkgJEmNLBCSpEYWCElSo+9bOl+GrPfY8AAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["# Inference test set\n","print(\"******Start inference on test set******\")\n","start_2 = time.time()\n","inference(test_loader)\n","end_2 = time.time()\n","time_to_train_2 = (end_2 - start_2)/60\n","print(\"Total Inference time to train on GPU (min):\", time_to_train_2)\n","\n","print(\"*****===== EVERYTHING WORKED FINE AND FINISHED HERE !!!!!!!!!!!=====*****\") "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447,"referenced_widgets":["49879c885f814fe4bd212dcb397dc1b2","a1a19950aef44ddba37ec534b4059695","556544f37a0343199278477b814f2b75","d2094f7042e24334bf6f403ed1404423","abc8edb9de964a7c8fa6f949fdad48a5","6325b36008a941f2bba05363629aa142","10007e275b1847049902a1bab93378c5","63bd031212534fd98c3a0ed859e4c724","d04f2a32deb84eef8b0104fcbdb6b3ed","9a09ed2baeff4104883bfa30296c63dc","5ad48f9ecf1e4913a42d9f99c9c4d08d"]},"id":"bDSD5zGXggVl","executionInfo":{"status":"error","timestamp":1652892427430,"user_tz":-420,"elapsed":21,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}},"outputId":"79c017fb-2beb-42a1-b95c-176195585da5"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["******Start inference on test set******\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49879c885f814fe4bd212dcb397dc1b2"}},"metadata":{}},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-379f89bffa99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"******Start inference on test set******\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mend_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtime_to_train_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend_2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-b1564ebb5cae>\u001b[0m in \u001b[0;36minference\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'=============='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-dc7c3e29a36d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/conv/graph_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight, size)\u001b[0m\n\u001b[1;32m     80\u001b[0m         out = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n\u001b[1;32m     81\u001b[0m                              size=size)\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_rel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mx_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/dense/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"K0K-o2g5HkTU","executionInfo":{"status":"aborted","timestamp":1652892427438,"user_tz":-420,"elapsed":10,"user":{"displayName":"Tân Phạm Ngọc","userId":"00919428611691747476"}}},"execution_count":null,"outputs":[]}]}